# -*- Org -*-
# 
# Copyright (c) 2021, Hiroyuki Ohsaki.
# All rights reserved.
# 

# This document is licensed under a Creative Commons
# Attribution-NonCommercial-ShareAlike 4.0 International License (CC
# BY-NC-SA 4.0).

# This document is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# Creative Commons License for more details.

# You should have received a copy of the license along with this work.
# If not, see <http://creativecommons.org/licenses/by-nc-sa/4.0/>.

* 文字の表現
<<ch:char>>

** 文化・慣習による表記の違い

私たちの普段の日常生活では、
ひらがな、
カタカナ、
漢字、
数字、
記号、
アルファベットなど、
さまざまな *文字* が使用されています。

[[ch:number]] 章で説明した数の表現は、
世界中のどのコンピュータでもほぼ同じ表現が使われています。
データの大きさが 16 ビットであり、
データで表現したい数が自然数であれば、
世界中どこでも 16 ビットの *符号なし整数* として表現することが一般的です。
また、
データの大きさが 16 ビットであり、
データで表現したい数が負の数も含んだ整数であれば、
世界中どこでも 16 ビットの *符号付き整数 (2 の補数)* として表現することが一般的です。
同様に、
データで表現したい数が実数であれば、
*固定小数点数* か *浮動小数点数* として表現するのが普通です。
もちろん例外もありますが、
多くの場合、
浮動小数点数であれば IEEE 754 に従った形式で表現します。

数学はその地域の文化や慣習にほとんど依存しない、
世界共通の概念です。
そのためコンピュータにおける数の取り扱いも、
世界中で共通の取り扱いができます。
もし仮に、
- 欧米では数の符号はプラスとマイナスの 2 種類である
- 東アジアでは数の符号は 3 種類ある (ただし日本と韓国には 4 種類ある)
- ヨーロッパでは数の符号は 7 種類のうち 2 つが使われる (どれが使われるかは国による)
のような不幸な状況であったとしたら、
世界中で数の表現はバラバラになったでしょう。
しかし幸いにして、
(少なくとも現代では) 世界中どこに行っても数の符号は 2 種類 (プラスとマイナスのみ) です。

ただし数の表記に関しては文化や慣習に依存しているところもあります。
例えば、
日本、
米国、
イギリスでは、
1234567.89 を
#+begin_quote
1,234,567.89
#+end_quote
のように表記します。
3 桁ごとにカンマ (~,~) で区切って、
小数点にはピリオド (~.~) を使います。
小数点は英語で decimal point です。
文字通り小数の (decimal) 点 (point) です。

一方、
フランス、
ドイツ、
イタリアなどの国では、
#+begin_quote
1.234.567,89
#+end_quote
のように表記します。
3 桁ごとにピリオド (~.~) で区切って、
小数の区切りにはカンマ (~,~) を使います。
小数の区切りにピリオド (点) を使わないので、
区切り文字を小数点と呼ぶのも不適切です。
カンマによる小数の区切りを英語で decimal comma と呼びます。
日本語に訳せば「小数カンマ」です。

例えば、
日本では 1,234 は 4 桁の整数を意味します。
一方、
フランスでは 1,234 は 1 桁の整数部と 3 桁の小数部で構成される実数を意味します。
日本、
米国、
イギリスなどの文化圏で生活している人にとっては、
小数点がピリオドなのは常識であり、
同じように、
フランス、
ドイツ、
イタリアなどの文化圏で生活している人にとっては小数の区切りがカンマ (小数カンマ) なのが常識です。

ただし、
幸いなことに、
こういった数の表現方法の違いは、
機械語やアセンブリ言語のレベルでは意識する必要がありません。
[[sec:number/binary-real]] 節で述べたように、
固定小数点数であっても浮動小数点数であっても、
「小数の区切りに何を使うか」とは関係ないデータ構造になっています。
桁の区切りや小数の区切りにどの記号を使うかはユーザインターフェースによります。
オペレーティングシステムやアプリケーションが、
ユーザに対してどのように数を表示するか、
ユーザから入力された数をどのように解釈するかが問題で、
CPU のレジスタやメモリに数をどのように格納するかとは直接関係がありません。

またこれも幸いなことに、
1. コンピュータは米国生まれである
2. 日本と米国は同じ数の表記法を使っている
ことから、
コンピュータの世界では小数の区切りにピリオドを使うことが大半です。
オペレーティングシステムやアプリケーションプログラムのレベルでは小数の区切りを切り替えられるものもありますが、
アセンブリ言語、
C 言語、
C++ 言語、
Python 言語など、
ほとんどのプログラミング言語では小数点 (ピリオド) しか使用できません。

日本の文化圏で生活しているわれわれにとっては幸いなのですが、
例えばフランスで生活している人にとっては、
以下のようなトラブルが日常的に起こっているのかもしれません。

#+begin_quote
今日は C 言語でラジアンを弧度法に変換するプログラムを書こう。
角度を \theta ラジアンとすると、
弧度法への変換は
\begin{align}
  360 \times \frac{\theta}{\pi}
\end{align}
だ。
円周率 \pi は 3,141592\dots だから、
これを素直に C 言語で書くとこうなる (図 [[fig:char/rad2deg.c]])。

#+caption: char/rad2deg.c
#+label: fig:char/rad2deg.c
#+include: "code/char/rad2deg.c" src C

よしよし。
早速コンパイルしよう。
#+begin_src C
$ gcc -Wall rad2deg.c 
rad2deg.c: In function ‘rad2deg’:
rad2deg.c:6:17: error: expected identifier or ‘(’ before numeric constant
   double pi = 3,141592;
                 ^~~~~~
rad2deg.c:7:15: error: invalid type argument of unary ‘*’ (have ‘double’)
   return 360, * theta / pi;
               ^~~~~~~
rad2deg.c:7:13: warning: left-hand operand of comma expression has no effect [-Wunused-value]
   return 360, * theta / pi;
             ^
rad2deg.c: In function ‘main’:
rad2deg.c:13:19: error: too many arguments to function ‘rad2deg’
   printf ("%f\n", rad2deg (1,23));
                   ^~~~~~~
rad2deg.c:4:1: note: declared here
 rad2deg (double theta)
 ^~~~~~~
rad2deg.c: In function ‘rad2deg’:
rad2deg.c:8:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
#+end_src

何だろうこの大量のエラー?
ああ、
そうだった。
C 言語では小数の区切りはピリオドを使わないとダメだった……
#+end_quote

数の表記法の違いはロケール (locale) として国際化対応が進んでいます。
GNU/Linux であれば、
環境変数 ~LC_NUMERIC~ を設定することにより数値の表記法を切り替えることができます。

環境変数 ~LC_NUMERIC~ を ~en_US.UTF-8~ にすれば、
小数の区切りはピリオド、
桁の区切りはカンマになります。
#+begin_src raw
$ export LC_NUMERIC=en_US.UTF-8		← ロケールを指定
$ printf "%'f\n" 1234.5
1,234.500000 ← 桁の区切りはカンマ、小数の区切りはピリオド
$ seq 0.1 0.1 1		← 0.1〜1 まで 0.1 刻みで出力するコマンド
0.1	← 小数の区切りはピリオド
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
$ du -sh /bin
7.4M    /bin	← これも小数の区切りはピリオド
#+end_src

また、
環境変数 ~LC_NUMERIC~ を例えば ~de_DE.UTF-8~ にすれば、
小数の区切りはカンマ、
桁の区切りはピリオドになります。
#+begin_src raw
$ export LC_NUMERIC=de_DE.UTF-8		← ロケールを指定
$ printf "%'f\n" 1234.5
bash: printf: 1234.5: invalid number	← 1234.5 を与えるとエラーになる!
1.234,000000
$ printf "%'f\n" 1234,5		← 小数の区切りにカンマを使わないといけない
1.234,500000	← 桁の区切りはピリオド、小数の区切りはカンマになった
$ seq 0.1 0.1 1
0,1	← seq コマンドの出力も変化した
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1,0
$ seq 0,1 0,1 1		← 0,1 を与えるとエラーになる!
seq: invalid floating point argument: ‘0,1’
Try 'seq --help' for more information.
$ du -sh /bin
7,4M    /bin	← これも小数の区切りはカンマ
#+end_src

上の例では、
printf コマンドの入力も出力も小数カンマです。
しかし、
seq コマンドの入力は小数点 (ピリオド) ですが、
出力は小数カンマです。
このように、
1. それぞれのプログラムがロケールに対応しているか
2. 対応しているとしてもどの程度対応しているか
3. 対応しているとしても入力をどう扱うか
などまちまちです。
そのため、
#+begin_quote
このプログラムでこれをするときには小数の区切りにカンマを使って、
あのプログラムであれをするときには小数の区切りにピリオドを使って、
あっちのプログラムであれをするときにも小数点の区切りにピリオドを使って……
#+end_quote
のように、
どのプログラムで何をしたいかによって、
そのつどルールを使い分ける必要があります。
小数点の区切りにカンマを使っている国や地域の方々は大変ですね。

数の表現でも十分大変ですが、
文字の表現は、
数の表現とは比べものにならないくらい複雑です。
文字は、
その地域の文化や慣習によってまったくといっていいほど異なるからです。

ただしこれも幸いにして、
先人たちの長年の苦労によって、
コンピュータにおける文字の取り扱いが *Unicode (ユニコード)* にほぼ統一されつつあります。
しばらく前までは互換性のない文字の表現形式が、
それぞれの国や地域の事情によって乱立していました。
日本国内だけでも、
複数の文字エンコーディングが乱立していました。
1991 年から標準化が始まった Unicode は、
世界中で使用されているさまざまな文字を統一的に扱える規格として発展および普及しています。

以下では、
初期のコンピュータにおける文字の表現から、
Unicode による文字の表現まで説明します。

アセンブリ言語でプログラムを書くだけであれば、
多くの場合、
文字コードに関する深い理解は必要ありません。
C 言語や Java 言語と同じように、
一般的なプログラムを書くだけであれば英数字と記号だけが使えれば十分です。
プログラム中のコメントも英語で書くなら、
日本語入力システムさえ不要です。

ただし、
文字情報を含んだバイナリファイルやバイナリデータを扱うプログラムを書いたり読んだりする場合には、
文字の表現に対する理解が不可欠です。
文字コードに対する理解は、
アセンブリ言語に限らず、
さまざまなプログラムを作成するときに役立つことでしょう。

** 文字に関する用語
<<sec:char/terminlogy>>

コンピュータにおける文字の表現を正しく理解するためには、
まず、
コンピュータにおける文字の表現に関する用語を正しく理解することが重要です。

英数字と記号に加えて、
いくつかの制御文字だけが使用できる文字コードの体系である ASCII を理解するだけならそれほど難しくありません。
しかし、
日本語を構成するひらがな、
カタカナ、
漢字を表現するためには、
ASCII と比べるとはるかに複雑な文字コードを理解する必要があります。

文字の表現に限りませんが、
コンピュータサイエンスの分野では、
名前は似ているけれども意味が大きく異なる言葉が複数あります。
それらの言葉が場合によっては混同して誤用されていることもあり、
「結局どういうことなのか」というのがわかりづらい状況です。
例えば、
文字の表現に関して言えば、
- 「文字」「キャラクタ」
- 「文字列」「ストリング」
- 「文字集合」「キャラクタセット」
- 「文字コード」
などの、
よく似た言葉が使われます。

ここからは、
まず、
コンピュータにおける文字の表現に関する用語を説明していきましょう。

**** 文字 (キャラクタ)

*文字* とは、
書き言葉における最小の構成要素であり、
何らかの意味を持ったものを指し \cite{Unicode:Glossary}、
英語では *character* と呼びます。
英語の character には「特徴、
性格、
登場人物」などの意味もありますが、
ここでは「文字」という意味の character です。

#+begin_note
letter という英語も文字のことを指しますが、
letter は主にアルファベットを構成する文字を意味します。
#+end_note

日本語では文字と *キャラクタ* は同じ意味で使われます。
単に character を漢字で表記しているか、
外来語としてカタカナ表記しているかの違いです。
このため、
「文字エンコーディング」と「キャラクタエンコーディング」は同じ意味です。

コンピュータにおける「文字」は、
数字、
英字、
記号、
ひらがな、
カタカナ、
漢字、
さらには他の言語で使用されている多様な文字も含みます。
また、
コンピュータにおける文字は *制御文字* も含みます。

英字の「A」、
ひらがなの「あ」、
漢字の「亜」、
記号の「%」や「。」
などもすべて 1 つの文字です (表 [[tab:char/char]])。
これらに加えて、
*タブ* や
*改行* などの制御文字も文字として扱います。
制御文字は画面に表示できませんし、
紙に印刷しても何も表示されませんが文字の一種として扱います。

#+caption: 文字の例
#+label: tab:char/char
| 1         |
| A         |
| あ        |
| ア        |
| 亜        |
| ~%~       |
| ~。~      |
| <空白>    |
| <タブ>    |
| <改行>    |

多くの場合、
文字の字体 (スタイルやデザイン) によらず、
同じ意味を持つ文字は 1 つの文字と見なします。
例えば、
「亜」という文字が、
明朝体でもゴシック体でも、
また、
大きな文字でも小さな文字でも、
同じ「亜」という文字と見なすことが一般的です。

「文字」そのものは抽象的な概念で、
具体的にコンピュータでその文字をどのように格納したり、
表示したりするのかとは直接関係ないことに注意してください。
例えば、
大文字のアルファベットを構成する文字は 26 個ありますが、
それぞれを何ビットで表現するかや、
文字の順序関係をどのように定義するかは、
後述するエンコーディングによって決まります。

**** 文字列

1 つまたは複数の文字の並びを *文字列 (string)* と呼びます (表 [[tab:string]])。
英語の string は「ひと続きに連なったもの」という意味であり、
コンピュータにおける string は、
「文字がひと続きに連なったもの」という意味になります。

#+begin_note
文字が 1 つもない場合 (0 個の文字の並び) を文字列に含める場合もあります。
#+end_note

#+caption: 文字列の例
#+label: tab:string
| 1                                    |
| 123                                  |
| 1,234.56                             |
| A                                    |
| A<空白>computer                      |
| <空白>                               |
| <タブ><タブ><タブ><改行>             |
| あ                                   |
| ああ、そうでした。<改行>             |
| <タブ>計算機<タブ>コンピュータ<改行> |

「文字」と同じように「文字列」も抽象的な概念であり、
単に、
「文字が順序関係を持って並んだもの」という意味です。
1 文字を何バイトで表現するとか、
文字の並びをどのように格納するとか、
文字列の終端をどのように識別するのかとは無関係です。

**** 文字集合

*文字集合 (character set)* とは、
その名のとおり文字 (character) の集合 (set) です。
ここでの set は、
「置く、
配置する」という意味の set (動詞) ではなく、
「ひとまとまり、
集合」という意味の set (名詞) です。

上の文字や文字列と同じように、
文字集合には「単なる文字の集まり」という意味しかありません。
英大文字 26 個を集めれば、
それは英大文字の文字集合です。
ひらがな、
カタカナ、
常用漢字をすべて集めれば、
それはひらがな・カタカナ・常用漢字の文字集合です。
単なる集合なので、
やはり文字と文字の順序関係など関係ありません (定義されません)。

通常、
文字集合には、
何らかの書き言葉を構成するすべての文字を含めます。
どの文字を含めて、
どの文字を含めないかによって文字集合は異なりますが、
英語だと数十文字程度、
日本語だと数万文字程度の大きさになります。
実用的な文字集合ではありませんが、
文字集合とは何かを理解してもらうための人工的な例を表 [[tab:char/char-set]] に示します。
文字集合は単なる集合ですので、
並び順に意味はないことに注意してください。

#+caption: 文字集合のサンプル
#+label: tab:char/char-set
| 0 1 2 3 4 5 6 7 8 9 + - . ,                                 |
| A B C D E F G H I J K L M N O P Q R S T U V W X Y Z         |
| あ い う え お                                              |
| あ か さ た な は ま や ら わ ア カ サ タ ナ ハ マ ヤ ラ ワ |
| <空白> <タブ> <改行> <フォームフィード>                     |
| 大 第 台 代 題 内 醍 臺 迺 餒 戴 岱 廼 梯 夬 夭 夲          |

**** エンコーディング

*エンコーディング (encoding)* とは、
何らかのデータを、
一定の規則に従って変換することを指し、
日本語では *符号化* とも呼びます。
コンピュータは情報処理装置であり、
文字や文字列の変換に限らず、
コンピュータにおけるさまざまなデータの変換をエンコーディングと呼びます。
ファイルの圧縮や暗号化、
画像やビデオの符号化などもエンコーディングの一種です。

文字の表現におけるエンコーディングは、
*文字エンコーディング (character encoding)* を意味します。
つまり、
「文字の情報を、
ある一定の規則に従って何らかのビット列に変換すること」です。
character encoding を漢字で表せば「文字符号化」です。
エンコーディングだと何のことかピンと来ないかもしれませんが、
「文字符号化」と聞けば、
何となくニュアンスが理解できると思います。

代表的な (日本でよく使われる) エンコーディング (= 文字符号化) には、
- ASCII
- ISO-8859-1
- UTF-8
- EUC-JP
- ISO-2022-JP
- シフト JIS
などがあります。

エンコーディングは抽象的な概念ではなく、
「それぞれの文字を、
コンピュータで扱えるように、
どのようなビット列で表すか」という具体的なものです。
例えば、
漢字の「亜」を UTF-8、
UTF-16、
EUC-JP、
ISO-2022-JP、シフト JIS で表現 (符号化) すると、
表 [[tab:char/encoding]] のようなバイト列になります。

#+caption: 代表的なエンコーディングにおける「亜」のバイト列 (文字コード)
#+label: tab:char/encoding
| エンコーディング | 「亜」の文字コード            |
|------------------+-------------------------------|
| ASCII            | 表現できない                  |
| ISO-8859-1       | 表現できない                  |
| UTF-8            | 0xe4 0xba 0x9c                |
| UTF-16 LE        | 0x9c 0x4e                     |
| UTF-16 BE        | 0x4e 0x9c                     |
| EUC-JP           | 0xb0 0xa1                     |
| ISO-2022-JP      | 0x30 0x21 (7 ビット \times 2) |
| シフト JIS       | 0x88 0x9f                     |

表現したい文字と使用するエンコーディングが決まれば、
その文字を符号化したビット列が定まります。
また逆に、
符号化されたビット列と、
符号化に用いたエンコーディングが決まれば、
表現された文字が定まります。

エンコーディングは、
文字を *一定の規則に従ってビット列に変換すること* を意味します。
ASCII エンコーディングの変換規則は、
ASCII の仕様書に書かれており、
同じく UTF-8 エンコーディングの変換規則は、
UTF-8 の仕様書に書かれています。

[[ch:number]] 章で述べた数の表現と同じように、
ビット列だけを見てもこれが何の文字を表しているかは *わからない* ことに注意してください。
例えば、
整数をコンピュータで表現する場合も、
符号なし整数か符号付き整数かはビット列を見ても判断できませんでした。
これと同じように、
ビット列だけを見ても、
どのようなエンコーディングなのかがわからなければ表現されている文字はわかりません。

#+begin_note
それぞれのエンコーディングには特徴がありますので、
ある程度長いビット列があれば使用されたエンコーディングを推定することは可能です。
ただし、
あくまで推定ですので、
必ずしも正しく判定できるわけではありません。
#+end_note

**** 文字コード

*文字コード (character code)* とは、
本来、
ある特定の文字 (character) に定められた *符号 (code)* のことです。
character code を漢字で表現すれば「文字符号」となります。

例えば、
ASCII において、
英大文字の「A」の文字コードは 0x41 です。
より正確には、
ASCII は 7 ビットのエンコーディングなので、
英大文字の「A」の文字コードは 1000001\posn{2} となります。
つまり、
ASCII において、
ある特定の文字「A」に定められた符号が 1000001\posn{2} なのです。

同様に、
EUC-JP において、
漢字「亜」に定められた符号が 0xb0、
0xa1 という 2 バイト (16 ビット) です (表 [[tab:char/encoding]])。

残念ながら日本では、
*文字エンコーディング (character encoding)* と *文字コード (character code)* という言葉が混同して使用されることが多いようです。
前述のように、
文字コードとは、
それぞれの文字に対して定められた符号です。

例えば、
以下のような表現を頻繁に目にしますが、
論理的には誤りです。
#+begin_quote
(誤) 添付ファイルの文字コードはシフト JIS です
#+end_quote
こういった場合は、
カタカナ語を漢字に直すとよくわかります。
#+begin_quote
(誤) 添付ファイルの文字符号はシフト JIS です
#+end_quote
ファイルは「文字」ではありませんし、
シフト JIS は「符号」でもありません。
正しくは以下のようになります。
#+begin_quote
(正) 添付ファイルの(文字)エンコーディングはシフト JIS です
#+end_quote

**** CCS (Coded Character Set)

*CCS (Coded Character Set)* は日本語にすれば *符号化文字集合* です。
RFC 2978 \cite{RFC2978} によれば、
CCS とは、
「抽象的な文字の集合から整数の集合への 1 対 1 対応のマッピング」と定義されています。
とはいえ、
何のことかよくわからないと思います。

こういう場合は、
その名前から「そもそもそれが何なのか?」
を理解しましょう。
CCS は Coded Character Set の略です。
先頭の「Coded」を取ると単なる「Character Set」となり、
上で説明した文字集合 (character set) と同じです。
また、
CCS の末尾の S は set であり、
結局のところ CCS は「集合」だといえます。

先頭の「Coded」は「符号化された、
コード化された」という意味の形容詞のため、
CCS は *文字集合だが、
特にコード化されたもの* という意味だとわかります。

文字集合の例 (表 [[tab:char/char-set]]) と同じように、
人工的な例で CCS を説明しましょう。
例えば
#+begin_quote
あ い う え お
#+end_quote
という、
あ行のひらがな 5 文字で構成される文字集合を例にします。
文字集合なので、
*順番には意味がありません*。
そのため、
#+begin_quote
う え お い あ
#+end_quote
と書いても同じ文字集合です。

さて、
CCS はコード化された文字集合です。
「あ」〜「お」に、
それぞれ 16 進数で 0x2422 から始まる偶数のコードを割り当ててみましょう。
| 文字 | コード |
|------+--------|
| あ   | 0x2422 |
| い   | 0x2424 |
| う   | 0x2426 |
| え   | 0x2428 |
| お   | 0x242a |
これが CCS の一例です。
文字集合であって、
それぞれの文字にバラバラの整数を割り当てたもの、
というわけです。

#+begin_note
なぜ偶数のコードを割り当てるのか?
と思う方もいるかもしれません。

結論から言えば、
別に偶数である必要はなく、
CCS としては、
それぞれ異なる整数であればどんな値でもかまいません。
どんな値でもよいのですが、
ここでは JIS X 0208 と同じコードを割り当てています。
#+end_note

CCS と次の CES (文字エンコーディング体系) を組み合わせることによって、
文字→バイト列へのエンコーディングが可能になります。

**** 文字エンコーディング体系 CES (Character Encoding Scheme)

*CES (Character Encoding Scheme)* は日本語にすれば *文字エンコーディング体系* です。
最後の単語が「scheme」なので、
集合 (set) でも文字 (character) でもなく、
CES は「scheme (体系)」だとわかります。
英語の scheme は日本語に訳しづらい言葉ですが、
ここでは以下のような意味です \cite{OALD9}。
#+begin_quote
scheme noun \\
1. (British English) a plan or system for doing or organizing something
#+end_quote

RFC 2978 \cite{RFC2978} によれば、
CES とは「単一もしくは複数の符号化文字集合 CCS から、
オクテット列の集合へのマッピング」とされています。

オクテットは通信用語で 8 ビットのことでした ([[sec:number/binary]] 節)。
先ほどの人工的な CCS を使って、
CES の例を示します。
| 文字 | コード |    | バイト列   |
|------+--------+----+------------|
| あ   | 0x2422 | → | 0xa4, 0xa2 |
| い   | 0x2424 | → | 0xa4, 0xa4 |
| う   | 0x2426 | → | 0xa4, 0xa6 |
| え   | 0x2428 | → | 0xa4, 0xa8 |
| お   | 0x242a | → | 0xa4, 0xaa |
ここで、
左側の CCS と、
右側のバイト列との対応関係を定めるものが CES です。
CES は文字エンコーディング体系なので、
CCS の各文字のコードから、
それに対応するバイト列への変換方法を定めています。
上の例では、
#+begin_quote
1. 文字のコードの上位 8 バイトに 0x80 を足したものを 1 バイト目とする
2. 文字のコードの下位 8 バイトに 0x80 を足したものを 2 バイト目とする
#+end_quote
という規則で、
文字のコードからバイト列へと対応させていてます。

#+begin_note
ここで示した CES の例は EUC-JP エンコーディング (のマルチバイト文字の扱い) と同じです。
#+end_note

** ASCII 
<<sec:char/ascii>>

*ASCII (American Standard Code for Information Interchange)* は米国規格協会 *ANSI (American National Standards Institute)* が策定した、
データ通信のための *文字コードの体系* です。
ASCII は 7 ビットのコード体系であり、
0x00〜0x7f (0〜127) の 128 種類の文字が含まれています。
ASCII は 1963 年に最初の策定がなされ、
その後、
1967 年から 1986 年までの間に何度か改訂されています。

#+begin_note
標準化を行った団体の名称が途中で 2 度変更になったため、
それにともなって規格の名称も変更になっています。
当初は ASA (American Standards Association; 米国標準規格協会) でしたが、
1967 年に USASI (United States of America Standards Institute; 米国規格協会) になり、
その直後の 1969 年に ANSI に名前が変更になりました。
#+end_note

ASCII を規定しているのは以下の規格です。
- ASA X3.4-1963
- USAS X3.4-1967
- USAS X3.4-1968
- ANSI X3.4-1977
- ANSI X3.4-1986
現在、
広く使われているのは 1968 年に発行された *X3.4-1968* です。
1963 年に発行された X3.4-1963 は、
英小文字がないなど、
現在利用されている ASCII とは異なるため、
ここからは X3.4-1968 を中心に説明します。

現代のコンピュータで使用されている 8 ビットの文字コードのほとんどは ASCII を含んでいます。
8 ビットの文字コードとして、
ラテン文字を含んだ *ISO 8859-1* や、
日本語の文字コードである EUC コードやシフト JIS コードが有名ですが、
これらの文字コードの前半部分 (0x00〜0x7f) には ASCII が (ほぼそのままの形で) 含まれています。
したがって、
ASCII において、
英大文字「A」の文字コードは 0x41 (= 65) ですが、
多くの 8 ビットの文字コードでも英大文字「A」の文字コードは 0x41 です。

ASCII は、
もともと *テレプリンタ (teleprinter; 電信印刷機)* のための文字コードとして設計されました。
筆者もテレプリンタの実物は見たことがないのですが、
写真を見る限り、
タイプライタの入力部分と印字部分を分離し、
それらを通信回線 (ケーブルや電話線) で接続したもののようです。

#+begin_note
テレプリンタの別名がテレタイプ (teletype) であり、
テレタイプの略称が TTY (TeleTYpe) です。
UNIX が生まれた当初、
端末としてテレタイプを使用していたことから、
UNIX では今でも端末のことを TTY と呼んでいます。
#+end_note

まず、
ASCII の文字コード表 (表 [[tab:char/ascii]]) を眺めてみましょう。

#+caption: ASCII-1968
#+label: tab:char/ascii
|    | 00  | 10  | 20 |     30 | 40 | 50 | 60 | 70    |
|----+-----+-----+----+--------+----+----+----+-------|
| 00 | NUL | DLE | SP |      0 | @  | P  | `  | p     |
| 01 | SOH | DC1 | !  |      1 | A  | Q  | a  | q     |
| 02 | STX | DC2 | "  |      2 | B  | R  | b  | r     |
| 03 | ETX | DC3 | #  |      3 | C  | S  | c  | s     |
| 04 | EOT | DC4 | $  |      4 | D  | T  | d  | t     |
| 05 | ENQ | NAK | %  |      5 | E  | U  | e  | u     |
| 06 | ACK | SYN | &  |      6 | F  | V  | f  | v     |
| 07 | BEL | ETB | '  |      7 | G  | W  | g  | w     |
| 08 | BS  | CAN | (  |      8 | H  | X  | h  | x     |
| 09 | HT  | EM  | )  |      9 | I  | Y  | i  | y     |
| 0a | LF  | SUB | *  |      : | J  | Z  | j  | z     |
| 0b | VT  | ESC | +  |      ; | K  | [  | k  | {     |
| 0c | FF  | FS  | ,  |      < | L  | \\ | l  | \vbar |
| 0d | CR  | GS  | -  | \equal | M  | ]  | m  | }     |
| 0e | SO  | RS  | .  |      > | N  | ^  | n  | ~     |
| 0f | SI  | US  | /  |      ? | O  | _  | o  | DEL   |

#+attr_latex: :environment longtable
| NUL | Null                                           |
| SOH | Start of Heading (CC)                          |
| STX | Start of Text (CC)                             |
| ETX | End of Text (CC)                               |
| EOT | End of Transmission (CC)                       |
| ENQ | Enquiry (CC)                                   |
| ACK | Acknowledge (CC)                               |
| BEL | Bell (audible or attention signal)             |
| BS  | Backspace (FE)                                 |
| HT  | Horizontal Tabulation (punched card skip) (FE) |
| LF  | Line Feed (FE)                                 |
| VT  | Vertical Tabulation (FE)                       |
| FF  | Form Feed (FE)                                 |
| CR  | Carriage Return (FE)                           |
| SO  | Shift Out                                      |
| SI  | Shift In                                       |
| DLE | Data Link Escape (CC)                          |
| DC1 | Device Control 1                               |
| DC2 | Device Control 2                               |
| DC3 | Device Control 3                               |
| DC4 | Device Control 4 (Stop)                        |
| NAK | Negative Acknowledge (CC)                      |
| SYN | Synchronous Idle (CC)                          |
| ETB | End of Transmission Block (CC)                 |
| CAN | Cancel                                         |
| EM  | End of Medium                                  |
| SUB | Substitute                                     |
| ESC | Escape                                         |
| FS  | File Separator (IS)                            |
| GS  | Group Separator (IS)                           |
| RS  | Record Separator (IS)                          |
| US  | Unit Separator (IS)                            |
| DEL | Delete                                         |

#+attr_latex: :environment maxtabular
| 文字   | 名称 \cite{RFC20}                                       | 日本語通用名称(参考) \cite{JISX0201} |
|--------+---------------------------------------------------------+--------------------------------------|
| SP     | Space (Normally Non-Printing)                           |                                      |
| ~!~    | Exclamation Point                                       | 感嘆符                               |
| ~"~    | Quotation Marks (Diaeresis)                             | 引用符                               |
| ~#~    | Number Sign                                             | 番号記号                             |
| ~$~    | Dollar Sign                                             | ドル記号                             |
| ~%~    | Percent                                                 | パーセント                           |
| ~&~    | Ampersand                                               | アンパサンド                         |
| ~'~    | Apostrophe (Closing Single Quotation Mark Acute Accent) | アポストロフィー                     |
| ~(~    | Opening Parenthesis                                     | 左小括弧                             |
| ~)~    | Closing Parenthesis                                     | 右小括弧                             |
| ~*~    | Asterisk                                                | アステリスク                         |
| ~+~    | Plus                                                    | 正符号                               |
| ~,~    | Comma (Cedilla)                                         | カンマ                               |
| ~-~    | Hyphen (Minus)                                          | ハイフン、負符号                     |
| ~.~    | Period (Decimal Point)                                  | ピリオド                             |
| ~/~    | Slant                                                   | 斜線                                 |
| ~:~    | Colon                                                   | コロン                               |
| ~;~    | Semicolon                                               | セミコロン                           |
| ~<~    | Less Than                                               | 不等号 (より小)                      |
| ~=~    | Equals                                                  | 等号                                 |
| ~>~    | Greater Than                                            | 疑問符                               |
| ~?~    | Question Mark                                           | 不等号 (より大)                      |
| ~@~    | Commercial At                                           | 単価記号                             |
| ~[~    | Opening Bracket                                         | 左大括弧                             |
| ~\~    | Reverse Slant                                           |                                      |
| ~]~    | Closing Bracket                                         | 右大括弧                             |
| ~^~    | Circumflex                                              | アクサンシルコンフレックス           |
| ~_~    | Underline                                               | アンダライン                         |
| ~`~    | Grave Accent  (Opening Single Quotation Mark)           | アクサングラーブ                     |
| ~{~    | Opening Brace                                           | 左中括弧                             |
| \vbar  | Vertical Line                                           | 縦線                                 |
| ~}~    | Closing Brace                                           | 右中括弧                             |
| =~=    | Overline (Tilde; General Accent)                        | オーバライン                         |

前半の 32 文字 (0x00〜01f) は *制御文字 (control character)* です。
NUL や SOH などの制御文字の種別が書かれていますが、
これらの制御文字は ~A~ や ~$~ のような図形を表示するためのものではありません。
例えば、
制御文字 0x01 (SOH) を端末エミュレータ上で出力しても、
画面には何も表示されません。

それ以降の 95 文字 (0x20〜0x7f) はすべて図形を表示するための *グラフィック文字 (graphic character)* です。
ただし、
最後の 0x7f (DEL) だけは再び制御文字です。

0x20〜02f には主要な記号が配置されています。
特に、
0x21〜0x29 までの記号の配列は JIS キーボードの数字キーの記号配列と同じです。
つまり、
JIS キーボードのメインキーにおいて、
| シフト + 1 | → | ! |
| シフト + 2 | → | " |
| シフト + 3 | → | # |
| シフト + 4 | → | $ |
| シフト + 5 | → | % |
| シフト + 6 | → | & |
| シフト + 7 | → | ' |
| シフト + 8 | → | ( |
| シフト + 9 | → | ) |
という配列は ASCII の 0x21〜029 に対応しているのです。
このため、
ASCII における記号 (~!~〜~)~ の範囲) の文字コードを知りたい場合、
JIS キーボードの数字キーを見ればわかります。
例えば、
1. ~&~ の文字コードが知りたい
2. JIS キーボードを見ると ~&~ は 6 キーにある
3. したがって ~&~ の文字コードは 0x20 + 6 = 0x26 だ
のようにわかります。

0x30〜0x39 には、
*数字の 0〜9* が配置されています。
そのため、
数字の文字コードは「0x30 + 数」と覚えておきましょう。
例えば、
~3~ の文字コードを知りたければ、
0x30 + 3 = 0x33 で、
「0x33」であることがわかります

0x41〜0x5a には *英大文字 A〜Z* が配置されています。
多くのプログラミング言語では配列のインデックスは 0 から始まるので、
コンピュータやプログラミングに慣れた人であれば、
英大文字 A〜Z を 0x41 ではなく、
0x40 から配置するほうが自然に感じると思います。
しかし、
ASCII では英大文字は 0x40 ではなく 0x41 から配置されています。
あくまで想像になりますが、
- A は 1 文字目のアルファベット
- C は 3 文字目のアルファベット
- N は 14 文字目のアルファベット
からすぐに文字コードが計算できるように、
英大文字を 0x41 から配置したのかもしれません。
英大文字の文字コードを知りたければ、
- A は 1 文字目のアルファベットだから、A の文字コードは 0x40 + 1 = 0x41
- C は 3 文字目のアルファベットだから、C の文字コードは 0x40 + 3 = 0x43
- N は 14 文字目のアルファベットだから、N の文字コードは 0x40 + 14 = 0x4e
のように計算することができます。

#+begin_note
文献 \cite{Mackenzie89:Codec} には ASCII の標準化の過程でどんな検討が行われたかが記述されています。
英大文字を 0x40 の列に配置することが決まったあとに、
大文字の A を 0x40 列の何番目に配置するか (0x40、
0x41、
0x42 \dots のどこに配置するか) の議論が行われたそうです。
標準化の過程では、
イギリスや ECMA (European Computer Manufacturers Association; 欧州コンピューター工業会) で策定されていた文字コード標準 \cite{ECMA-1} のドラフトにあわせて、
0 番目ではなく 1 番目に決定したのだそうです (ただし文献 \cite{Mackenzie89:Codec} の著者も、
イギリスや ECMA の文字コード標準がなぜ 1 番目を採用したのかはわからないそうです)。
#+end_note

0x61〜0x7a には *英小文字 a〜z* が配置されています。
#+begin_quote
英小文字の文字コード = 英大文字の文字コード + 0x20
#+end_quote
と、
キレイな配置になっています。

例えば、
大文字の A は 0x41、
小文字の a は 0x61 です。
これらを 2 進数で表記すると、
\begin{align}
  41\posn{16} = & 01000001\posn{2} \\
  61\posn{16} = & 01100001\posn{2}
\end{align}
のように *1 ビットだけが異なっている* ことがわかります。
ASCII では、
大文字と小文字が 1 ビット違いになるように意図して配置されています \cite{Mackenzie89:Codec}。
#+begin_quote
多くの記号の文字コード (空白を含む) < 英大文字の文字コード < 小文字の文字コード
#+end_quote
という関係が成り立っていることを覚えておきましょう。
そのため、
例えば
- multi-path transport protocol
- multi paths
- multipath transport protocol
- Multiple-Input and Multiple-Output
のような ASCII で符号化された 4 つの文字列がある場合、
これらを単純に文字コードのままソートすると
- Multiple-Input and Multiple-Output
- multi paths
- multi-path transport protocol
- multipath transport protocol
という順番に並びます。
「小文字より大文字が先に来て、
アルファベットよりも (多くの) 記号が先に来る」という、
それなりに自然な順番で並ぶことになります。

#+begin_note
「英字 A の ASCII コードは 0x41」のような表現がよく用いられますが、
論理的には少し変な表現です。

ASCII は American Standard Code for Information Interchange の略称ですので、
ASCII そのものが「米国標準コード」という意味です。
このため「ASCII コード」は「米国標準コードコード」という意味になってしまいます。

ANSI 以外の代表的な文字コードとして、
例えば
- ISO 8859-1
- JIS X 0201
- シフト JIS
- EUC-JP
- UTF-8
などがありますが、
これらの文字コードのあとに「コード」を足した、
- ISO 8859-1 コード
- JIS X 0201 コード
- シフト JIS コード
- EUC-JP コード
- UTF-8 コード
という表現は使いません。
「英字 A の UTF-8 コードは U+0041」が変な表現であるのと同じ理由で、
「英字 A の ASCII コードは 0x41」も変な表現だといえます。
#+end_note

** 制御文字
<<sec:char/ctrl>>

ASCII の文字コード表 (表 [[tab:char/ascii]]) を見ると、
#+begin_quote
7 ビットの文字コードなので、
表現できる文字数がたったの 128 文字しかない。
それなのに、
なぜ 33 文字 (0x00〜0x1f、
0x7f) も制御文字に割り当ててしまったのだろう?
制御文字の多くは使われていないので大変な無駄ではないか?
#+end_quote
と思うかもしれません。
筆者もそう思います。

実は、
ASCII がテレプリンタのための文字コードとして設計されたため、
このようになっています。
テレプリンタの制御のためにはこれらの多数の制御文字は有用でした。
しかし、
ASCII をコンピュータで利用する場合には、
多くの制御文字が意味をなさなくなってしまいました。

とはいえ、
一部の制御文字は現在でも使用されています (表 [[tab:char/ctrl]])。
制御文字の意味は、
元の制御文字の意味をそのまま引き継いでいるものもあれば、
元の制御文字の意味から一部変わったものもあります。

#+caption: ASCII の代表的な制御文字
#+label: tab:char/ctrl
#+attr_latex: :environment maxtabular
| コード | 略称 | 表記 | C言語の表記 | 名称               | 本来の意味                                 | 現在の主な用途                               |
|--------+------+------+-------------+--------------------+--------------------------------------------+----------------------------------------------|
|   0x00 | NUL  | ~^@~ | ~\0~        | ヌル(空)文字       | 意味のない文字                             | 文字列の終端                                 |
|   0x07 | BEL  | ~^G~ | ~\a~        | ベル               | 受信側のベルを鳴らす                       | ビープ音を鳴らす、画面を点滅する             |
|   0x08 | BS   | ~^H~ | ~\b~        | バックスペース     | 印刷位置を一文字戻す                       | 直前の文字に上書きする                       |
|   0x09 | HT   | ~^I~ | ~\t~        | 水平タブ           | 印刷位置を右方向に次のタブストップまで移動 | カーソルを次のタブストップまで右に移動       |
|   0x0a | LF   | ~^J~ | ~\n~        | ラインフィード     | 印刷位置を一行進める                       | テキストファイルの改行                       |
|   0x0b | VT   | ~^K~ | ~\v~        | 垂直タブ           | 印刷位置を下方向に次のタブストップまで移動 | カーソルを次のタブストップまで下に移動       |
|   0x0c | FF   | ~^L~ | ~\f~        | フォームフィード   | 印刷位置を次のページの先頭まで移動         | 画面を消去 (クリア) する                     |
|   0x0d | CR   | ~^M~ | ~\r~        | キャリッジリターン | 印刷位置を行頭に戻す (行送りはなし)        | カーソルを行頭に移動、テキストファイルの改行 |
|   0x1a | SUB  | ~^Z~ | ~\033~      | 置換               | 次の文字を置換                             | ファイルの終端 (MS-DOS)                      |
|   0x1b | ESC  | ~^[~ | ~\e~        | エスケープ         | 次の文字をクォート                         | エスケープシーケンスの開始・終了             |
|   0x7f   | DEL  |      | ~\177~      | デリート           | 削除された文字                             | カーソルの文字を削除、バックスペース         |

制御文字の機能を知っていると、
ちょっとしたプログラムを書くときに役立ちます。
このため、
それぞれの制御文字の機能を簡単な利用例とともに説明します。

なお、
端末に制御文字を出力したときに、
画面の表示がどのように変化するか (もしくは変化しないか) は、
その端末の機能や設定によります。
主要な端末エミュレータでは、
以下で紹介するような制御文字の多くはサポートされているはずです。

*** ヌル文字 (0x00, NUL, ~^@~, ~\0~)

制御文字 0x00 (NUL) は C 言語における *文字列の終端文字* です。

C 言語で以下のようなプログラム ~abc.c~ を記述した場合を考えましょう (図 [[fig:char/abc.c]])。
このように ~char~ 型の配列を宣言すると、
配列 ~str~ の長さは (3 ではなく) 4 になります。

#+caption: char/abc.c
#+label: fig:char/abc.c
#+include: "code/char/abc.c" src C

ここで、
ファイル ~abc.c~ を GCC でコンパイルし、
~objdump~ コマンドを使ってオブジェクトファイルの中身を見てみましょう (~objdump~ コマンドの使い方は [[sec:gas/binutils]] 節で説明します)。
次のように、
文字列を格納している .data セクションの中身を表示してみます。

#+begin_src raw
$ gcc -fno-pic -fomit-frame-pointer -c -o abc.o abc.c
$ objdump -s -j .data abc.o

abc.o:     file format elf32-i386

Contents of section .data:
 0000 41424300                             ABC.  
#+end_src

文字列 ~ABC~ が 4 バイト (0x41、
0x42、
0x43、
0x00) に変換されたことがわかります。
0x41、
0x42、
0x43 がそれぞれ「A」「B」「C」の文字コードであり (表 [[tab:char/ascii]])、
末尾の 0x00 が文字列の終端を表すヌル文字です。

*** ベル (0x07, BEL, ~^G~, ~\a~)

端末に制御文字 0x07 (BEL) を表示すると *ベル* が鳴ります。
端末の機能や設定によりますが、
ビープ音が鳴ったり、
画面全体もしくはその一部がフラッシュ (一時的に反転) します。
画面のフラッシュはビジュアルベル (visual bell) と呼ばれます。
#+begin_src sh
$ cat /bin/ls
#+end_src
のように誤ってバイナリファイルを表示すると、
「ピッ、
ピピッ、
ピッ、
ピピピピッ」のようにビープ音が立て続けに鳴ります。
これは、
バイナリファイル中の 0x07 を表示した瞬間に音が鳴っているからです。

*** バックスペース (0x08, BS, ~^H~, ~\b~) 

端末に制御文字 0x08 (BS) を表示すると、
これも端末の機能や設定によりますが、
*カーソルが 1 文字後退* します (左に 1 文字だけ移動します)。

[[sec:number/tool]] 節で紹介した ~printf~ コマンドを使って、
制御文字が混ざった文字列を端末に出力してみましょう。
~printf~ コマンドは C 言語の関数 ~printf~ と同じエスケープ文字 (~\b~ や ~\n~ など) が利用できます。
#+begin_src sh
$ printf 'AB\bXC\n'
AXC
$ printf 'good bye\b\b\bmorning.\n'
good morning.
#+end_src
バックスペース ~\b~ の直前の文字が消えていることがわかります。
末尾の ~\n~ は後ほど説明する改行の制御文字です。

UNIX では、
制御文字 0x08 を、
一部の文字を強調したり、
一部の文字に下線を引いたりするために使用しています。

例えば、
~AB\bBC~ を端末に出力すると、
上の例のように ~ABC~ とだけ表示され、
バックスペースの直前の文字は単純に消されます。

#+begin_note
もともと制御文字 0x08 (BS) は印刷位置を 1 文字戻すという意味でした (表 [[tab:char/ctrl]])。
当時のプリンタに ~AB\bBC~ を送信すると、
1. 1 文字目の A を印字する
2. 2 文字目の B を印字する
3. 3 文字目が BS なので、印刷位置を1文字戻す
4. 4 文字目の B を印字する
5. 5 文字目の C を印字する
のように動作します。
A と C はそれぞれ一度だけ印字されますが、
B が同じ位置に 2 度印字されます。
このため @@latex:{\tt A{\bf B}C}@@ のように B が濃く強調されます。

同様に、
当時のプリンタに ~A_\bBC~ を送信すると、
1. 1 文字目の A を印字する
2. 2 文字目の _ を印字する
3. 3 文字目が BS なので、印刷位置を1文字戻す
4. 4 文字目の B を印字する
5. 5 文字目の C を印字する
のように動作します。
これにより、
「_」と「B」が重ねて同じ位置に印字されます。
このため @@latex:{\tt A{\underline B}C}@@ のように B に下線が引かれます。

テキストファイル中にバックスペースを埋め込むことにより、
特定の文字を強調したり、
特定の文字に下線を引いたりすることができます。
例えば、
~normal bold underline\n~ という文字列中の ~bold~ を強調し、
~underline~ に下線を引くには以下のようにします。
#+begin_src sh
$ printf 'normal b\bbo\bol\bld\bd _\bu_\bn_\bd_\be_\br_\bl_\bi_\bn_\be\n' | ul
#+end_src
これを実行すると @@latex:{\tt normal {\bf bold} \uline{underline}}@@ のように表示されます。
バックスペースを埋め込んだテキストを ~ul~ コマンドにパイプで接続することで、
バックスペースによる強調や下線を、
利用している端末に合ったエスケープシーケンスに変換しています。

~less~ コマンドや ~lv~ コマンドのような代表的なページャもバックスペースを解釈するので
#+begin_src sh
$ printf 'normal b\bbo\bol\bld\bd _\bu_\bn_\bd_\be_\br_\bl_\bi_\bn_\be\n' | less
#+end_src
や
#+begin_src sh
$ printf 'normal b\bbo\bol\bld\bd _\bu_\bn_\bd_\be_\br_\bl_\bi_\bn_\be\n' | lv
#+end_src
などでも強調や下線が表示されます。

#+end_note

*** 水平タブ (0x09, HT, ~^I~, ~\t~)

端末に制御文字 0x09 (HT) を表示すると、
端末の機能や設定によりますが、
多くの場合、
カーソルが *次の (水平) タブストップまで移動* します。

制御文字 0x09 は正確には「水平タブ」です。
しかし、
制御文字 0x0b の垂直タブがそれほど利用されないこともあり、
水平タブは単に *タブ* と呼ばれることがほとんどです。
本書でも、
単に「タブ」と表記していれば、
それは水平タブのことを意味します。

多くの環境では、
*タブストップ* は 8 カラムごとに設定されています。
そのためカーソルがタブ位置にあるときに制御文字 0x09 を出力すると、
カーソルが右に 8 文字分移動します。
そのためカーソルがタブ位置の次の文字にあるときに制御文字 0x09 を出力すると、
カーソルが右に 7 文字分移動します。
つまり、
制御文字 0x09 を表示すると、
現在のカーソルの位置に応じて、
カーソルが右に 1〜7 文字分移動します。

タブは非常に便利ですので、
ぜひ使いこなせるようになりましょう。

他の制御文字とは異なり、
タブは *スペースと同じ空白文字の仲間* です。
アセンブリ言語を始め、
多くのプログラミング言語ではスペースとタブをどちらも区切り文字として利用できます。

ASCII ではスペース (0x20) はグラフィック文字ですが、
水平タブ (0x09) は制御文字に分類されています。
ASCII の仕様策定の過程においても、
#+begin_quote
(タブと同じように) スペースは印刷位置を 1 文字進めるだけで何も印字しないが、
スペースは制御文字か、
それともグラフィック文字だろうか?
#+end_quote
という議論があったそうです \cite{Mackenzie89:Codec}。
最終的に、
スペースはグラフィック文字に分類されましたが、
そういった経緯もあって制御文字とグラフィック文字の境界である 0x20 に配置されています (表 [[tab:char/ascii]])。

このようにスペースとタブは、
どちらも制御文字とグラフィック文字の両方の役割を持っている (他の制御文字やグラフィック文字とは異なる) 文字です。

タブは、
もともとの役割が「表作成」であったこともあり、
プログラムからの出力を見やすくするために利用できます。

~/bin~ ディレクトリに格納されているファイルのファイル名、
ファイルの大きさ、
所有者を表示するには、
例えば ~awk~ コマンドを利用して
#+begin_src sh
$ ls -l /bin | awk '{ print $9, $5, $3 }' | head
  
bash 1168776 root
bunzip2 38984 root
bzcat 38984 root
bzcmp 6 root
bzdiff 2227 root
bzegrep 6 root
bzexe 4877 root
bzfgrep 6 root
bzgrep 3641 root
#+end_src
のようにします。
しかしこれではファイル名の長さがバラバラなので読みづらいため、
水平タブを使用して、
#+begin_src sh
$ ls -l /bin | awk '{ printf "%s\t%s\t%s\n", $9,$5,$3 }' | head
                
bash    1168776 root
bunzip2 38984   root
bzcat   38984   root
bzcmp   6       root
bzdiff  2227    root
bzegrep 6       root
bzexe   4877    root
bzfgrep 6       root
bzgrep  3641    root
#+end_src
のようにすればファイルの大きさと所有者のカラムがそろうため読みやすくなります。

もちろん、
水平タブは次のタブストップに移動するだけなので、
ファイル名の長さがバラバラであれば、
必ずしもカラムがキレイにそろいません。
例として、
s から始まるファイルの結果だけを抽出してみましょう。
#+begin_src sh
$ ls -l /bin | awk '{ printf "%s\t%s\t%s\n", $9,$5,$3 }' | grep '^s'
sed     122224  root
sh      4       root
sleep   39552   root
ss      161488  root
stty    80672   root
su      63568   root
sync    35488   root
systemctl       872792  root
systemd 20      root
systemd-ask-password    14520   root
systemd-escape  18496   root
systemd-inhibit 18520   root
systemd-machine-id-setup        26792   root
systemd-notify  18504   root
systemd-sysusers        55664   root
systemd-tmpfiles        80128   root
systemd-tty-ask-password-agent  30792   root
#+end_src

少し見づらいですが、
プログラムのデバッグに使用する場合などではこれでも十分です。
また、
上の出力は、
後ほど説明する TSV 形式になっていますので、
単に読みやすいだけでなく、
データの再利用にも好都合です。

多くの端末は端末エミュレータではタブストップは 8 文字ごとです。
ただし、
C 言語や Python 言語などでは 4 文字のインデントがよく利用されるため、
タブストップが 4 文字ごとのテキストエディタも広く利用されています。
タブ幅が 4 文字の環境で作成したテキストファイルを、
タブ幅が 8 文字の環境で表示するとレイアウトが乱れてしまいます。
例えば、
タブ幅が 4 文字のテキストエディタで以下のファイル ~sum.c~ を作成します (図 [[fig:char/sum-tab4.c]])。

#+caption: char/sum-tab4.c
#+label: fig:char/sum-tab4.c
#+include: "code/char/sum-tab4.c" src C

これをタブ幅が 8 文字の環境で開くと、
以下のようにインデントが崩れて表示されてしまいます。

#+include: "code/char/sum-expanded.c" src C

C 言語ではインデントは特別な意味を持たないため、
コンパイルすれば正しく動作はします。
ただし、
上記のプログラムを読むのも、
編集するのも大変です。

~cat~ コマンドに ~-T~ オプション (show tabs) を指定すれば、
制御文字であるタブが ~^I~ のように表示されます。
#+begin_src sh
$ cat -T sum.c
int
main(void) {
^Iint sum = 123;
^Ifor (int i = 0; i < 100; i++) {
^I  if (i % 3 != 0)
^I^I  sum += i;
^I}
^Ireturn 0;
}
#+end_src

また、
UNIX の ~expand~ コマンドや ~unexpand~ コマンドを利用すればタブの除去やタブストップの変換ができます。
例えば、
タブ幅が 4 文字の環境で作成されたファイルのタブを除去するには以下のように ~expand~ コマンドを使用します。
#+begin_src C
$ expand -t4 sum.c
int
main(void) {
    int sum = 123;
    for (int i = 0; i < 100; i++) {
      if (i % 3 != 0)
          sum += i;
    }
    return 0;
}
#+end_src
~-t4~ オプションを指定して、
タブ幅が 4 であることを指定しています。
~expand~ コマンドはタブをすべてスペースに置換します。

さらにタブ幅が 4 文字の環境で作成されたファイルを、
タブ幅が 8 文字に変換したいような場合、
~expand~ コマンドと ~unexpand~ コマンドを組み合わせます。
#+begin_src C
$ expand -t4 sum.c | unexpand >sum-tabbed.c
#+end_src
これにより、
タブ幅が 8 文字のファイル ~sum-tabbed.c~ が作成されます。
同じように、
タブ幅が 8 文字のファイルを、
タブ幅が 4 文字のファイルに変換することも可能です。
#+begin_src C
$ expand -t8 sum-tabbed.c | unexpand -t4 >sum-tabbed4.c
#+end_src

先述のように、
タブは制御文字とグラフィック文字の両方の側面を持っています。
このことから、
タブを区切り文字とした、
*TSV (Tab-Separated Values) 形式* と呼ばれる単純なテキストファイル形式が非常に便利に使えます。

TSV 形式とは、
表形式のデータをプレーンテキストとして格納するファイル形式です。
TSV 形式の例を示しましょう (図 [[fig:char/account.tsv]])。

#+caption: char/account.tsv
#+label: fig:char/account.tsv
#+include: "code/char/accoun.tsv" src raw

ファイルの中身を見ても、
ただのテキストファイルだと思うかもしれませんが、
実はフィールドの区切りがタブになっています。
~cat -T~ で見てみましょう。
#+begin_src sh
$ cat -T accoun.tsv
id^Iuid^Igid^Iname^Ihome^Ishell
root^I0^I0^Iroot^I/root^I/bin/sh
daemon^I1^I1^Idaemon^I/usr/sbin^I/usr/sbin/nologin
nobody^I65534^I65534^Inobody^I/nonexistent^I/usr/sbin/nologin
ohsaki^I1000^I1000^IHiroyuki Ohsaki^I/home/ohsaki^I/usr/bin/fish
#+end_src
各行が 1 つのレコードになっており、
各レコードを構成するフィールドがタブで区切られていることがわかります。
「普通のテキスト情報には (制御文字である) タブは含まれない」という性質を利用して、
タブをフィールドの区切り文字として使用しているのです。
また、
タブは空白の一種でもあるため、
TSV ファイルを表示するとそれなりに読み取れる形で表示されます。

なお、
TSV 形式ではフィールド中にタブや改行は使用できません。
フィールドの値がタブや改行などの特殊文字を含む場合は、
~\t~ や ~\n~ などの表記で個別に対応します。

表計算やデータベースなど、
異なるアプリケーション間で表形式のデータを交換する場合には、
途中にいったん TSV 形式に変換するようにすると大変便利です。
「ある形式→ TSV 形式」と「TSV 形式→ある形式」の変換プログラムを用意しておけば、
あらゆるファイル形式の間でデータの変換が可能になります。

TSV 形式に似たテキストファイル形式に *CSV (Comma-Separated Values)* 形式があり、
CSV 形式ではフィールドの区切り文字にカンマを使用します。

しかし、
カンマはグラフィック文字なので、
フィールド中のカンマか、
区切り文字のカンマかを区別する必要があります。
また、
CSV 形式はフィールド中の改行も許すため、
CSV 形式の読み込みはかなり複雑です。

例として、
同じ表形式のデータを TSV 形式と CSV 形式で示してみましょう。

#+caption: char/complex.tsv (TSV 形式)
#+label: fig:char/complex.tsv
#+include: "code/char/complex.tsv" src raw

#+caption: char/complex.csv (CSV 形式)
#+label: fig:char/complex.csv
#+include: "code/char/complex.csv" src raw

TSV 形式は非常に単純です。
ただし、
上でも説明したように、
TSV 形式ではフィールド中にタブや改行は使用できません。
ここでは、
フィールド中のタブは ~\t~ と表記する、
フィールド中の改行は ~\n~ と表記しています。
TSV 形式のファイルを読み込む側で、
これらのクォートされた文字をそれぞれ元に戻す必要があります。

CSV 形式は見るからに複雑です。
フィールド中の文字がカンマや改行である場合には、
それぞれの文字をクォートするのではなく、
フィールド全体を区切り文字 (例えば ~"~) で囲む必要があるからです。

TSV 形式のファイルを読み込むプログラムなら数行で書けますが、
CSV 形式のファイルを読み込むプログラムを書くのは大変です。
正規表現等を使っても CSV 形式のファイルを読み込むプログラムは簡単には書けません。

TSV 形式のファイルを読み込むプログラムの例を示します。
Perl と Python で書いてみました (図 [[fig:char/parse-tsv.pl]] および図 [[fig:char/parse-tsv.py]])。
どちらのプログラムも、
ファイルを 1 行ずつ読み込み、
1 行をタブで分割することによってフィールドに分解しています。
その後、
各フィールド中のクォートされた文字 (~\t~ および ~\n~) を元に戻しています。

#+caption: char/parse-tsv.pl
#+label: fig:char/parse-tsv.pl
#+include: "code/char/parse-tsv.pl" src perl

#+begin_src raw
$ ./parse-tsv.pl complex.tsv

### @tbl: [
###         [
###           '空白文字',
###           'カンマを含む',
###           'タブを含む',
###           '改行を含む'
###         ],
###         [
###           ' ',
###           '1,234.56',
###           'a        b',
###           'X
Y'
###         ]
###       ]
#+end_src

#+caption: char/parse-tsv.py
#+label: fig:char/parse-tsv.py
#+include: "code/char/parse-tsv.py" src python

#+begin_src raw
$ ./parse-tsv.py complex.tsv
[['空白文字', 'カンマを含む', 'タブを含む', '改行を含む'], [' ', '1,234.56', 'a\tb', 'X\nY']]
#+end_src

CSV 形式のファイルを読み書きする必要がある場合は、
そのためのモジュールやライブラリを使うほうがよいでしょう。
Python であれば標準の csv モジュールが利用できます。

#+include: "code/char/parse-csv.py" src python

#+begin_note
次のタブストップまで移動させる制御文字を、
なぜ「タブ (tab)」と呼んでるのかと疑問に思わないでしょうか?
英語の tab は以下のような意味です \cite{OALD9}。
#+begin_quote
tab noun
1. a small piece of paper, cloth, metal, etc. that sticks out from the
   edge of something, and that is used to give information about it,
   or to hold it, fasten it, etc
#+end_quote
日本語で言えば「つまみ、
ラベル、
付け札」といった意味です。
制御文字の水平タブには、
つまみ、
ラベル、
付け札といった役割はなさそうです。

実は、
水平タブは英語で horizontal tabulation と言います。
英語の tabulation は以下のような意味です。
#+begin_quote
tabu・la・tion noun [uncountable, countable] \\
the arrangement of facts or figures in columns or lists so that they
can be read easily; a table of facts or figures
#+end_quote
日本語で言えば「表にすること、
表作成」といった意味です。
日本語で言えば、
水平タブは「水平表作成」で、
タブキーは「表作成キー」です。

どちらも同じ「タブ」ですが、
タブキーのタブは tabulation (もしくは tabulator) の「タブ」、
タブブラウザ (表示するページを上部のタブで切り替えられる Web ブラウザ) のタブは tab (つまみ、
ラベル、
付け札) の「タブ」の意味です。
[[sec:asm/terminlogy]] 節でも述べたように、
カタカナ語は元となった英語表現を見ると、
多くの場合、
その概念を正しく理解できます。
#+end_note

*** ラインフィード (0x0a, LF, ~^J~, ~\n~)

端末に制御文字 0x09 (LF) を表示すると *改行* されます。
つまり、
カーソルが次の行の先頭に移動します。
カーソルが画面の一番下にある場合は、
通常、
画面全体が上に 1 行分スクロールし、
画面のスクロールによってできた空行の先頭にカーソルが移動します。

*ラインフィード (line feed)* という言葉は、
昔のタイプライタやテレタイプから来ています。
ライン (line) は印字する用紙の 1 行を意味し、
フィード (feed) は英語で「エサを与える、
食べさせる」という意味です。
タイプライタやテレタイプに対して、
「用紙を 1 行分飲み込んで」と指示するというニュアンスです。

当時のテレタイプ端末では、
制御文字 LF に対する動作が、
1. 単に用紙が1行送られるだけのもの (印字位置が一行下に移動するもの)
2. 用紙が1行送られたあと、キャリッジリターンするもの (印字位置が一行下
   の行頭に移動するもの)
の 2 種類があったそうです。
どちらのラインフィードの動作をもとにしたかによって、
複数のテキストファイルの改行の形式が生まれました。

UNIX 上のテキストファイルでは、
*改行は LF (0x0a) の 1 文字* で表現します。
C 言語の文字列の終端記号が NUL (0x00) でしたが、
UNIX におけるテキストファイルの行末記号が LF (0x0a) です。
つまり、
UNIX におけるテキストファイルは「1 行目の文字列、
LF、
2 行目の文字列、
LF \dots 最終行の文字列、
LF」という構成になっています。
つまり UNIX では、
ラインフィードを「改行 (行送り + 行頭に移動)」の意味で解釈している、
といえます。
そのため UNIX では、
制御文字 LF を画面に出力すると改行するし、
テキストファイル中の制御文字 LF も改行を意味しています。

一方、
CP/M や MS-DOS を起源とするオペレーティングシステム (例えば Microsoft Windows) 上のテキストファイルでは、
*改行は制御文字 CR (0x0d) と LF (0x0a) の 2 文字* です。
これらのオペレーティングシステムにおけるテキストファイルは「1 行目の文字列、
CR、
LF、
2 行目の文字列、
CR、
LF \dots 最終行の文字列、
CR、
LF、
SUB」という構成になっています (ファイル末尾の SUB については後ほど説明します)。
つまり、
これらのオペレーティングシステムでは、
ラインフィードを「単なる行送り」の意味で解釈している、
といえます。
キャリッジリターン CR (0x0d) で行頭に戻して、
その後、
ラインフィード LF (0x0a) で 1 行進めている、
というわけです。

改行コードが違うので、
以下のようなトラブルが頻繁に起こります。

1. UNIX で作成したテキストファイル (改行は LF のみ) を Windows で開く
   と、*長大な 1 行のみ* で構成されるテキストファイルに見える (CR + LF
   が 1 つもないので 1 行だけのテキストファイルと解釈される)
2. Windows で作成したテキストファイル (改行のは CR + LF) を UNIX で開
   くと、*すべての行の末尾に ~^M~ 等* が含まれてしまう (CR が行の一部
   と見なされて、CR (0x0d) が ~^M~ 等と表示される)
3. Windows で作成したテキストファイル (改行は CR + LF) を UNIX で開く
   と、各行の末尾に目に見えない制御文字が混入していて混乱する (CR が行
   の一部と見なされるが、CR は制御文字なので表示されない (のでゴミがあ
   ることに気付かない))

これらの中では、1.と 2.はまだよい (目で見れば大抵おかしなことが起こっていることに気付く) のですが、3.は厄介です。

今、
ここに 2 つのファイル ~lf.txt~ と ~crlf.txt~ があります。
~cat~ コマンドで中身を確認してみます。
#+begin_src sh
$ cat lf.txt
Hello, World!
$ cat crlf.txt
Hello, World!
#+end_src

このように、
どちらも Hello, World! とだけ書かれたファイルですが、
~diff~ コマンドを使って 2 つのファイルが一致するかを調べてみましょう。
#+begin_src sh
$ diff -u lf.txt crlf.txt
--- lf.txt      2021-03-20 16:29:31.101829110 +0900
+++ crlf.txt    2021-03-20 16:29:43.071840183 +0900
@@ -1 +1 @@
-Hello, World!
+Hello, World!
#+end_src
2 つのファイルはどう見てもまったく同じですが、
1 行目が異なっていると表示されました。
~diff~ コマンドの出力を見ても、
どこがどう違うのかまったくわかりません。

これが先ほどの 3.のパターンのトラブルです。
ファイル ~lf.txt~ は UNIX 形式のテキストファイル (改行は LF のみ) で、
ファイル ~crlf.txt~ は CP/M や MS-DOS 形式のテキストファイル (改行は CR+LF) です。
CR は制御文字であってグラフィック文字ではないため、
CR を出力しても画面上には何も見えません。
上の ~cat~ コマンドも ~diff~ コマンドも、
実際には CR を出力しているのですが制御文字なので見えないのです。

*** 垂直タブ (0x0b, VT, ~^K~, ~\v~)

端末に制御文字 0x0b (VT) を表示すると、
端末の機能や設定によりますが、
多くの場合、
カーソルが *次の (垂直) タブストップまで移動* します。
水平タブは右のタブストップに移動しますが、
垂直タブは *下のタブストップ* に移動します。

多くの環境では垂直方向のタブストップは 1 行ごとに設定されています。
そのため制御文字 0x0b を出力すると、
カーソルが下に 1 文字分移動します。
改行 (LF) ではカーソルが次の行の先頭に移動しますが、
垂直タブ (VT) では現在のカーソルの直下に移動します。

いろいろな場面で大活躍の水平タブとは異なり、
垂直タブはかなりマイナーな存在です。

あまり魅力的な例ではないかもしれませんが、
垂直タブを使えば、
例えばこんなことができます。
階段を描画するプログラムです。
#+include: "code/char/vtab.sh" src sh
#+begin_src raw
___
   |
   |
   |___
       |
       |
       |___
           |
           |
           |___
               |
               |
               |___
                   |
                   |
                   |
#+end_src

垂直タブ (VT) でカーソルを下方向に移動することができ、
バックスペース (BS) でカーソルを左方向に移動することができます。
グラフィック文字を出力するとカーソルが右に移動するので、
垂直タブ (VT) とバックスペース (BS) を組み合わせることで、
左・右・下にカーソルを移動させることができます。
あと、
カーソルを上に移動させることができればもっと複雑な描画もできるのですが、
残念ながら制御文字だけではできません (後述するエスケープシーケンスを使えば可能です)。

*** フォームフィード (0x0c, FF, ~^L~, ~\f~)

制御文字 0x0c (FF) は現在ではあまり使用されません。
これも端末の機能や設定によりますが、
制御文字 0x0c (FF) を出力しても何も表示されません。

ただし、
C 言語や Python 言語のようなプログラミング言語ではフォームフィードを空白文字として扱うことと、
フォームフィードにもともとページの区切りという役割があったことから、
ソースコード中の論理的な区切りを表すために利用されることもあります。

ラインフィードと同じように、
*フォームフィード (form feed)* という言葉も、
昔のタイプライタやテレタイプから来ています。
フォーム (form) は印字する用紙を表し、
フィード (feed) は、
ラインフィードの「フィード」と同じように、
英語で「エサを与える、
食べさせる」という意味を持ちます。
タイプライタやテレタイプに対して、
「残りの用紙を丸ごと飲み込んで」と指示するというニュアンスです。

テレタイプ端末がフォームフィードを受信すると、
印字する用紙を、
次のページの開始位置まで移動させることになり、
印刷位置を次のページの先頭まで移動させることになります。

つまり、
「フォームフィード (FF) があれば、
そこで現在のページは終わり、
そこから次のページが新しく始まる」ことを意味します。
このためフォームフィードは *改ページ (page break)* とも呼ばれます。

C 言語や Python 言語などのソースコード中にフォームフィード (FF) を埋め込んでおけば、
テキストエディタの機能によって前後のフォームフィードにジャンプすることができます。

例えば Python で複数のクラスからなる長いプログラムを記述しているときに、
クラス定義の先頭にフォームフィードを記入しておきます。
Python にとって *フォームフィードは単なる空白* なので、
フォームフィードが埋め込まれていてもプログラムの実行には影響しません。

しかし、
このソースコードをテキストエディタで編集しているときに、
前後のフォームフィードにジャンプすれば、
クラスの定義の先頭部分に即座にジャンプすることができます。
Emacs であれば forward-page (~C-x ]~)、
backward-page (~C-x [~) によって、
現在のカーソル位置 (Emacs ではポイントと呼びます) の前後のフォームフィードにジャンプすることができます。

*** キャリッジリターン (0x0d, CR, ~^M~, ~\r~)

端末に制御文字 0x0d (CR) を表示すると、
カーソルが *行頭に移動* します。
カーソルの次の行の行頭ではなく、
カーソルが現在位置している行の行頭に移動します。

*キャリッジリターン (carriage return)* という言葉も、
昔のタイプライタから来ています。
キャリッジ (carriage) はタイプライタの用紙を動かす部分の名称です。
タイプライタやでは、
アルファベットや数字の活字が刻印されたハンマーを紙に打ち付けることによって印字します。
活字が付いたハンマーが移動するのではなく、
用紙のほうを移動させるという仕組みになっています。
キャリッジリターンというのは、
キャリッジを戻す (return) という操作を意味しています。
つまり、
キャリッジを戻すことにより、
用紙を移動させることによって印刷位置を行頭に戻します。

現在のコンピュータの端末でも、
キャリッジリターンは当時のキャリッジリターンと同じように動作します。
つまり、
印刷位置 (カーソルの位置) を行頭に移動します。
キャリッジリターンのよくある利用法の 1 つは、
「プログラムの実行状態を表示するために用いる」というものです。
例を示しましょう (図 [[fig:char/cr.sh]])。

#+caption: char/cr.sh
#+label: fig:char/cr.sh
#+include: "code/char/cr.sh" src sh

これを実行すると
#+begin_src raw
$ ./cr.sh
Loading object files... 38%
#+end_src
のように表示されているパーセントの値がどんどん大きくなります。
値が変化するたびに改行するのではなく、
1 行中のパーセントの値だけが変化します。
プログラムの完了までに時間のかかる処理を行うときによく表示されるタイプの実行ステータス表示です。

同じ行に、
繰り返しプログラムの実行状態を表示するために、
キャリッジリターンによって毎回行頭に戻り、
直前に表示した実行状態の文字列を毎回上書きしています。
for ループを終了する時点でも、
キャリッジリターンによってカーソルが行頭に戻るため、
表示が乱れることなく ~Loading object files... Done~ という文字列を ~echo~ コマンドで表示できます。

キャリッジリターンの重要な用途の 1 つは CP/M や MS-DOS を起源とするオペレーティングシステムにおける改行記号としての役割です。
詳細についてはラインフィードの項を参照してください。

*** 置換 (0x1a, SUB, ~^Z~, ~\033~)

制御文字 0x1a (SUB) は現在ではあまり使用されません。
これも端末の機能や設定によりますが、
制御文字 01a (SUB) を出力しても何も表示されません。

制御文字 0x1a は CP/M や MS-DOS を起源とする一部のオペレーティングシステムにおいて、
テキストファイルの終端を表す記号として使用されています。
UNIX ではファイルの終端は ~^D~ によって入力しますが、
CP/M や MS-DOS を起源とする一部のオペレーティングシステムでは ~^Z~ によって入力します。

*** エスケープ (0x1b, ESC, ~^[~, ~\e~)

制御文字 0x1b (ESC) は、
*エスケープ (escape)* という名前が表すように、
ASCII の制御文字・グラフィック文字による通信から「抜け出す」ために使用されます。

これも端末の機能や設定によりますが、
制御文字 0x1b (ESC) から始まる *エスケープシーケンス (escape sequence)* によって、
ASCII の制御文字・グラフィック文字だけではできない特別の指示を伝えます。
エスケープシーケンスは日本語で言えば「脱出順序、
脱出文字列」です。

例えば、
#+begin_src sh
$ printf '\e[7m'
#+end_src
を実行すると画面の色が反転します。
背景が黒で、
文字が白なら、
反転して背景が白で、
文字が黒になります。

上の例では「0x1b (ESC)」と「~[7m~」という 3 文字 (合計 4 文字) のエスケープシーケンスです。
受信側 (端末) は「~\e[7m~」という 4 文字を受け取ったら、
画面の色を反転する、
という取り決めになっています。
このため、
この取り決めに従った特別な文字列 (脱出文字列 = エスケープシーケンス) を送ると、
「~\e[7m~」という 4 文字を画面に表示するのではなく、
画面の色を反転するだけで、
画面にはグラフィック文字を何も表示しません。

代表的なエスケープシーケンスは、
ASCII と同じく ANSI の規格である、
*ANSI エスケープシーケンス (ANSI X3.64)* です。
ANSI エスケープシーケンスを使えば、
ASCII の制御文字だけではできなかったさまざまな処理が可能になります。
例えば、
カーソルを画面上の自由な位置に移動させたり、
画面に表示されている一部の文字や文字列を消去したり、
画面全体を上下にスクロールしたり、
文字のフォントを太字・イタリック・下線付きに変更したり、
文字は背景の色を変えたりすることができます。

ANSI エスケープシーケンスによって、
文字の種類や色を変化させるサンプルです (図 [[fig:char/escape.sh]])。
このプログラムを実行すると図 [[fig:char/escape.sh]] のように、
フォントが太字となり、
さまざまな前景色・背景色で文字列が表示されます。

#+caption: char/escape.sh
#+label: fig:char/escape.sh
#+include: "code/char/escape.sh" src sh

#+caption: char/escape.sh の実行結果
#+label: fig:xterm
[[./code/char/xterm.png]]

このプログラムで使用しているエスケープシーケンスを表 [[tab:char/ansi-esc]] に示します。

#+caption: ~escape.sh~ で使用している ANSI エスケープシーケンス
#+label: tab:char/ansi-esc
| 文字列 | 効果                            |
|--------+---------------------------------|
| ~[0m~  | リセット (すべての属性を初期化) |
| ~[1m~  | 太字もしくは強調                |
| ~[7m~  | 反転 (前景色と背景色の入れ替え) |
| ~[30m~ | 前景色の変更 (黒)               |
| ~[31m~ | 前景色の変更 (赤)               |
| ~[32m~ | 前景色の変更 (緑)               |
| ~[33m~ | 前景色の変更 (黄)               |
| ~[34m~ | 前景色の変更 (青)               |
| ~[35m~ | 前景色の変更 (マゼンタ)         |
| ~[36m~ | 前景色の変更 (シアン)           |
| ~[37m~ | 前景色の変更 (白)               |

*** デリート (0x7f, DEL, ~\177~)

制御文字 0xff (DEL) は現在ではあまり使用されません。
これも端末の機能や設定によりますが、
制御文字 0xff (DEL) を出力しても何も表示されません。

表 [[tab:char/ascii]] からわかるように、
DEL 以外の制御文字は 0x00〜0x1f の前半部分にまとまっています。
制御文字 DEL だけが末尾の 0x7f に配置されています。

これは NUL (0x00) と DEL (0x7f) には、
どちらも *そのビットパターンに意味がある* からです。
NUL はすべてのビットが 0 である 0000000\posn{2} です。
DEL はすべてのビットが 1 である 1111111\posn{2} です。

ちなみに、
NUL を 0x00 に、
DEL を 0x7f に配置するというのは、
AT&T からの要望だったそうです \cite{Mackenzie89:Codec}。

当時、
コンピュータへのデータ入力には紙テープが使用されていました。
私は紙テープを実際に使ったことはないのですが、
紙テープとは、
幅 1 インチ (25.4mm) 程度の長い紙であり、
そこに実際にパンチで穴を空けて、
コンピュータに入力するビット列を記録していました。
なお、
紙テープには縦に 8 個の穴を空けることができた (1 列で 1 バイトを表現していた) ようです。

紙テープは、
実際にパンチで穴を空けるので、
穴の空け間違いがあると修正が面倒です。
特に、
紙テープの途中に 1 バイト挿入しようと思うと、
すでに開口している穴を全部どちらかにずらさなければなりません。
そこで、
あらかじめ紙テープ上に穴が開いていない箇所を用意しておいて、
あとから必要に応じて使えるようにしていました。
また、
間違いがあった場合には、
すべての穴を開口することで「ここは意味のないデータである」ということを表現していたそうです

NUL と DEL はこういった、
当時の紙テープの利用法から来ています。
つまり、
すべてのビットが 0 であれば (縦 1 列に穴がなければ) 何も記録されていないとして扱い、
これが制御文字 NUL (0x00) です。
また、
すべてのビットが 1 であれば (縦 1 列すべて穴が開いていれば) 無効なデータとして扱い、
これが制御文字 DEL (0x1f) です。

** Unicode と UTF-8
<<sec:char/utf-8>>

ASCII の次は、
日本で利用されている *8 ビットエンコーディング* である JIS X 0201 (英数字・記号・半角カナ) を説明し、
その後、
ひらがな・カタカナ・漢字も含んだ *マルチバイト文字のエンコーディング* である EUC-JP、
シフト JIS、
ISO-2022-JP などを説明するというのが王道でしょう。

しかし本書執筆時点 (2021 年) では、
これまで広く利用されてきた、
EUC-JP、
シフト JIS、
ISO-2022-JP などの日本語の文字エンコーディングが急速に利用されなくなっています。

1980 年代から、
複数の日本語文字エンコーディングが乱立し、
かなり混乱した状況が続きましたが、
現在は、
世界中の言語で使用されている文字を統一的に扱う規格である *Unicode* や、
Unicode の文字エンコーディング体系である *UTF-8* が広く使われています。
そこで本書では、
Unicode と UTF-8 を中心に説明します。

*** Unicode

Unicode とは、
Unicode コンソーシアムにおいて標準化されている文字コード体系の規格で、
「世界中で使われているすべての文字を統一的に扱う」という、
非常に野心的な規格です。

英語と日本語だけを見ても、
そこで使われる文字がまったくといっていいほど異なります。
英語では少数のアルファベットと数字・記号くらいしか使用されませんが、
日本語ではひらがな・カタカナに加えて、
多数の漢字が使用されます。
英語はすべて横書き (左から右) のみですが、
日本語は横書き (左から右) だけでなく縦書き (上から下) もあります。

Unicode は、
このような *世界中の言語で使用されているあらゆる文字* を対象としています。
例えば右から左に書くような横書きや、
複数の文字が複雑に組み合わされるような言語にも対応しています。

Unicode は世界中のあらゆる文字を扱うことを目指しているため、
それなりに複雑な規格です。
また、
Unicode の規格は、
新しく登場した文字を収録したり、
これまで収録されていなかった古代の文字を新たに収録したりするなど、
常に更新され続けています。

#+begin_note
Unicode バージョン 1.0 が策定されたのは 1991 年ですが、
本書執筆時点 (2021 年) の最新版は Unicode バージョン 13.0 です。
#end_note

*** Unicode の符号化文字集合

Unicode の規格には、
*符号化文字集合 (CCS)* と *文字エンコーディング体系 (CES)* の両方が含まれますが、
ここではまず符号化文字集合について説明します。

Unicode では、
それぞれの文字に割り当てられている整数を *コードポイント (code point)* と呼び、
例えば、
「あ」のコードポイントは 0x3042、
「亜」のコードポイントは 0x4e9c となります。
Unicode では、
コードポイントを表す際に、
*先頭に U+ を付けて*、
U+3042、
U+4E9C のように表記します。

Unicode のコードポイントの範囲は U+0000〜U+10FFFF です。
0x0000 から 0x10ffff までの範囲であり、
Unicode で表現できる文字は 1 114 112 種類です。
Unicode バージョン 13.0 に収録されている文字数が約 14 万なので、
まだまだコードポイントには空きがあるといえます。

Unicode のコードポイントは 17 個の *プレーン (plane)* に分かれています (表 [[tab:char/unicode-plane]])。
プレーンごとに、
どのような文字を収録するのかが決まっています。
それぞれのプレーンの大きさは 16 ビット (65 536 文字) であり、
例えば、
0 番目のプレーンは U+0000〜U+FFFF、
1 番目のプレーンは U+10000〜U+1FFFF、
2 番目のプレーンは U+20000〜U+2FFFF に対応しています。

#+caption: Unicode のプレーン
#+label: tab:char/unicode-plane
| プレーン番号 | 名称                                      |
|--------------+-------------------------------------------|
|            0 | BMP (Basic Multilingual Plane)            |
|            1 | SMP (Supplementary Multilingual Plane)    |
|            2 | SEP (Supplementary Ideographic Plane)     |
|            14 | SSP (Supplementary Special-purpose Plane) |
|       15, 16 | Private Use Plane                         |

0 番目のプレーン U+0000〜U+FFFF は *BMP (Basic Multilingual Plane)* と呼ばれます。
日本語で言えば「基本多言語面」です。
世界中の言語で使われている主要な文字の多くは 0 番目のプレーンに配置されています。

Unicode はコードポイントは U+0000〜U+10FFFF の範囲なので、
コードポイントの表現には 21 ビットが必要です。
当初、
Unicode は、
世界中のあらゆる文字を 16 ビット (U+0000〜U+FFFF) に格納することを目指していました。
しかし、
16 ビット (65 536 文字) には収まらないことが明らかになり、
結果的には 21 ビットになりました。
しかし、
当初 16 ビットに収めることを目指していただけあり、
U+0000〜U+FFFF には主要な文字の多くが配置されています。
われわれが普段日常的に使用するような、
ひらがな、
カタカナ、
漢字、
数字、
アルファベット、
記号などの大半は BMP に配置されています。

1 番目のプレーン U+10000〜U+1FFFF は *SMP (Supplementary Multilingual Plane)* と呼ばれます。
日本語で言えば「補助多言語面」です。
SMP には、
BMP に収録できなかったか、
もしくは使用頻度が低い文字や記号が配置されています。

例えば、
U+10000〜には図 [[fig:char/unicode-u10000]] のような文字が配置されています。
(少なくとも筆者にとっては) どんな言語の文字かも想像がつかないような文字が並んでいます。
Unicode の規格には、
Linear B Syllabary と記載されています。
どうやら、
Linear B (線文字 B) という、
紀元前 14〜12 世紀頃に使用されていた、
古代ギリシア語の音節文字だそうです。

#+caption: Unicode のコードポイント U+10000〜 (https://www.unicode.org/charts/PDF/U10000.pdf)
#+label: fig:char/unicode-u10000
[[./figure/char/unicode-u10000.png]]

また U+11000〜には図 [[fig:char/unicode-u11000]] のような文字が配置されています。
漢字の仲間のように見えますが、
どんな言語の文字かよくわかりません。
Unicode の規格には Tangut と記載されています。
これはどうやら、
Tangut (タングート) と呼ばれる、
6〜14 世紀にかけて活躍したチベット系民族で使用されていた表意文字のようです。

#+caption: Unicode のコードポイント U+11000〜 (https://www.unicode.org/charts/PDF/U11000.pdf)
#+label: fig:char/unicode-u11000
[[./figure/char/unicode-u11000.png]]

このように、
日常的に利用される文字は 0 番目のプレーンである BMP にそのほとんどが配置されています。

2 番目のプレーン U+20000〜U+2FFFF は *SIP (Supplementary Ideographic Plane)* と呼ばれます。
日本語で言えば「補助表意文字面」です。
ここには CJK (China-Japan-Korea; 中国・日本・韓国) で使用される文字のうち、
BMP に配置されなかった文字が SIP に配置されています。
めったに使用されない文字や、
歴史的な意味しかないような文字が配置されています。

例えば、
U+20000〜には図 [[fig:char/unicode-u20000]] のような文字が配置されています。
漢字の仲間であることはわかりますが、
(少なくとも筆者にとっては) 日本で使われている漢字なのかどうかもわからないような漢字が並んでいます。

#+caption: Unicode のコードポイント U+20000〜 (https://www.unicode.org/charts/PDF/U20000.pdf)
#+label: fig:char/unicode-u20000
[[./figure/char/unicode-u20000.png]]

14 番目のプレーン U+E0000〜U+EFFFF は *SSP (Supplementary Special-purpose Plane)* と呼ばれます。
日本語で言えば「補助特定用途面」です。
ここには BMP に配置できなかったフォーマット制御文字が配置されています。

15 番目および 16 番目のプレーン U+F0000〜10FFFF は *プライベート利用のためのプレーン* です。
つまり、
利用者が自由に文字を割り当てて使用してよい領域です。

上で説明した以外のプレーン (3〜13 番目のプレーン) は予約されています。

*** Unicode の文字エンコーディング体系

このように、
Unicode では、
世界中のあらゆる文字に U+0000 から U+10FFFF までのコードポイントを割り当てています。

ただし、
Unicode はある程度複雑な規格なので、
文字と Unicode のコードポイントが必ずしも *1 対 1 に対応していません*。
例えば、
ひらがなの「が」は U+304C ですが、
「か」 + 「゛」と見なして U+304B、U+3099 とも表記できます。
つまり、
ひらがなの「が」に対応する符号は、
U+304C と U+304B、U+3099 の 2 種類が存在します。

Unicode では、
アクセントや、
濁音、
半濁音が付いた文字などは、
複数の表現があることを覚えておいてください。
Unicode で表現された文字列に対して検索やパターンマッチングを行うときには、
複数の表現があることに注意が必要です。

#+begin_note
本書では詳しく説明しませんが、
Unicode の文字列に対して検索を行う場合には、
あらかじめ *正規化 (normalization)* を行ってから処理します。
#+end_note

(ある文字に対するコードポイントが必ずしも 1 つに決まらないという複雑さはありますが) 表現したい文字が決まれば、
その文字に対応したコードポイントが決まります。

[[sec:char/terminlogy]] 節の用語で言えば、
Unicode における「文字→コードポイント」の対応関係は符号化文字集合を定めていると言えます。
Unicode に収録されているすべての文字が文字集合であり、
それらの文字に対するコードポイントが割り当てられた符号です。

Unicode を使って文字をバイト列として表現するためには、
符号化文字集合に加えて、
文字エンコーディング体系が必要になります。
例えば、
「~s = "あ"~」という 7 文字には
| s      | U+0073 |
|        | U+0020 |
| \equal | U+003D |
|        | U+0020 |
| "      | U+003D |
| あ     | U+3042 |
| "      | U+003D |
というコードポイントが対応します。
これらのコードポイントの値を、
どのようにバイト列に変換するかを決める必要があります。

*** UTF-32

最も素朴な文字エンコーディング体系が *UTF-32 (Unicode Transformation Format-32)* です。
UTF-32 では、
Unicode のコードポイントを *すべて 32 ビットで* 表現します。

先述のように、
Unicode のコードポイントは U+0000〜U+10FFFF の値を取るため、
コードポイントの表現に必要なビット数は 21 ビットです。
この UTF-32 では、
コードポイントの表現に 32 ビットを使用します。
上位 11 ビットは必ずゼロになるという贅沢な表現形式です。

例えば、
先ほどの「~s = "あ"~」という 7 文字の例では
| s      | U+0073 | 0x0000 0073 |
|        | U+0020 | 0x0000 0020 |
| \equal | U+003D | 0x0000 003d |
|        | U+0020 | 0x0000 0020 |
| "      | U+003D | 0x0000 003d |
| あ     | U+3042 | 0x0000 3042 |
| "      | U+003D | 0x0000 003d |
であることから、
32 ビット (4 バイト) \times 7 文字として、
0x0000 0073、
0x0000 0020、
0x0000 003d、
0x0000 0020、
0x0000 003d、
0x0000 3042、
0x0000 003d のように表現します。

文字の大半は 0 番目のプレーンである BMP に配置されているので、
ほとんどの場合、
*上位の 16 ビットはゼロ* になります。
またこの例では英数字や記号が中心ですので、
ひらがなの「あ」を除いて下位 7 ビットしか使用されません。
大変贅沢な (無駄の多い) エンコーディングだといえます。

32 ビットの値の列をバイト列に変換するためにはあと手間必要です。
32 ビットの値をビッグエンディアンで表現するのか、
リトルエンディアンで表現するのかによって、
2 種類の表現形式 (*UTF-32BE (Big Endian)* と *UTF-32LE (Little Endian)*) があります (エンディアンネスについては [[sec:mem/endianness]] 節で説明します)。

上の例を UTF-32BE で表現すると、上位バイトを先に並べて
#+begin_src sh
00 00 00 73 00 00 00 20 00 00 00 3d 00 00 00 20 
00 00 00 3d 00 00 30 42 00 00 00 3d
#+end_src
となります。同様に、UTF-32LE で表現すると、下位バイトを先に並べて
#+begin_src sh
73 00 00 00 20 00 00 00 3d 00 00 00 20 00 00 00 
3d 00 00 00 42 30 00 00 3d 00 00 00
#+end_src
となります。

*** UTF-16

UTF-32 はシンプルでわかりやすいのですが、
記憶領域の効率という点では非常に無駄の多いエンコーディング形式です。

日常的に利用される文字の大部分は 0 番目のプレーンである BMP に配置されるので、
ほとんどの文字は U+0000〜U+FFFF の 16 ビットで表現できるはずです。
そこで、
Unicode のコードポイントを、
それぞれ 16 ビットで表現するという形式として *UTF-16 (Unicode Transformation Format-16)* が定められています。

例えば、
先ほどの「s = " あ "」という 7 文字の例では、
すべての文字が BMP に配置されていました。
したがって、
UTF-16 では、
| s      | U+0073 | 0x0073 |
|        | U+0020 | 0x0020 |
| \equal | U+003D | 0x003d |
|        | U+0020 | 0x0020 |
| "      | U+003D | 0x003d |
| あ     | U+3042 | 0x3042 |
| "      | U+003D | 0x003d |
のように、
16 ビット (2 バイト) $\times$ 7 文字として、
0x0073、
0x0020、
0x003d、
0x0020、
0x003d、
0x3042、
0x003d のように表現します。

32 ビットではなく 16 ビットで各コードポイントを表現しているため、
当然ですが、
UTF-32 と比べるとデータの大きさが半分に (28 バイトから 14 バイトに) なりました。
ただし、
この例では英数字や記号が中心なので、
ひらがなの「あ」を除いて下位 7 ビットしか使用されていません。

UTF-32 をバイト列で表現する方法として UTF-32BE や UTF-32LE が存在するのと同じように、
UTF-16 をバイト列で表現する方法として *UTF-16BE* や *UTF-16LE* が存在します。
32 ビットの値をビッグエンディアンで表現するのか、
リトルエンディアンで表現するのかによって 2 種類があります。

上の例を UTF-16BE で表現すると、
上位バイトを先に並べて
#+begin_quote
00 73 00 20 00 3d 00 20 00 3d 30 42 00 3d
#+end_quote
となります。
同様に、
UTF-16LE で表現すると、
下位バイトを先に並べて
#+begin_quote
73 00 20 00 3d 00 20 00 3d 00 42 30 3d 00 
#+end_quote
となります。

ただし、
Unicode のコードポイントは U+10FFFF まであるため、
必ずしもコードポイントは 16 ビットに収まりません。
1 番目以降のプレーンの文字が利用される頻度は低いといえますが、
それでも一部は使用されるかもしれません。

UTF-16 では、
1 番目以降のプレーンに配置された、
17 ビット以上を要するコードポイントは *サロゲートペア (surrogate pair)* として表現します。
英語の surrogate は「代わりの、
代理の」という意味であり、
英語の pair は「1 対、
1 組」という意味です。
このため、
surrogate pair を日本語に訳せば「代理の一組」といえます。

16 ビットで文字を表現しているときに、
16 ビットに収まらない他の文字を表現するときの常套手段は、
ISO-8859 や ISO-2022 のようにエスケープシーケンスを使うという方法です。
例えば、
1 番目のプレーンの文字を表現したいときには、
エスケープシーケンスを使って現在のプレーンを 0 番から 1 番に切り替えます。
1 番目のプレーンの文字の表現が終われば、
再度エスケープシーケンスを使用して現在のプレーンを 1 番から 0 番に切り替えます。

UTF-16 では、
これまで広く利用されてきたエスケープシーケンスを使用 *しません* 。
エスケープシーケンスを使ってプレーンを切り替えるのではなく、
1 番目以降のプレーンのコードポイントを、
*サロゲートペアという特殊な 2 文字* を使って表現します。

サロゲートペアは、
*前方サロゲート (leading surrogate)* と *後方サロゲート (trailing surrogate)* によって構成されます。
前方サロゲートは U+D800〜U+DBFF の範囲のコードポイントを取り、
後方サロゲートは U+DC00〜U+DFFF の範囲のコードポイントを取ります。
前方サロゲートの U+D800〜U+DBFF も、
後方サロゲートの U+DC00〜U+DFFF も 0 番目のプレーン (BMP) なのですが、
BMP の U+D800〜U+DBFF と DC00〜U+DFFF には文字が配置されておらず、
*サロゲート用に空けられて* います。

1 番目以降のプレーンの Unicode コードポイントは、
0 番目のプレーンの前方サロゲートと後方サロゲートの計 32 ビットで表現します。
Unicode コードポイントの (最上位ビットを除く) 上位 10 ビットを前方サロゲートで表現し、
下位 10 ビットを後方サロゲートで表現しています。
例えば、
1 番目のプレーンの U+10302 は、
前方サロゲート U+D800 と後方サロゲート U+DF02 の計 32 ビットで表現します。

Unicode は他の文字エンコーディングと比較して新しい規格であるため、
よく考えられたエンコーディングとなっています。
UTF-16 は、
EUC-JP やシフト JIS などの従来の 16 ビット文字エンコーディングと比較して以下のような利点を持っています。

- 16 ビット (65 536 文字) を超える、数百万種類の文字を表現できる
- ある特定の 16 ビットだけを見れば、その文字が BMP の文字か、前方サロ
  ゲートか、後方サロゲートであるかが判別できる
- そのため、*文字列の走査* (例えば文字数のカウント) を簡単に行える
  (EUC-JP やシフト JIS では、いったん前方まで戻ってから走査し直さな い
  といけない)
- ある特定の 16 ビットだけが壊れたとしても、*その1文字が破壊されるだ
  け* で済む (EUC-JP やシフト JIS では、最悪の場合、後続するすべての文
  字が破壊される可能性がある)

*** UTF-8

以上のように、
UTF-16 はかなり優れた文字エンコーディング体系だといえます。

そのため、
日本語のひらがな・カタカナ・漢字が中心のテキストであれば UTF-16 はよい選択肢となりますが、
プログラムのソースコードや、
インターネット上を伝送される通信プロトコルなど、
英数字や記号が中心であれば UTF-16 は記憶領域の効率という点ではまだまだ無駄の多いエンコーディング形式です。

ASCII や ISO-8859 のようなラテン文字が中心であれば、
多くの文字は 8 ビットに収まります。
Unicode の 0 番目のプレーンの先頭は ASCII と同じなので、
多くの文字は U+0000〜U+00FF の 8 ビットに収まります。

現在、
多くのオペレーティングシステムやアプリケーションは、
テキストファイルのデフォルトの文字エンコーディングとして *UTF-8* を使用しています。
他にも、
2021 年の時点で、
世界中の Web サイトの約 97% は文字エンコーディングとして UTF-8 を使用しているそうです。

UTF-32 や UTF-16 と比較したときの、
UTF-8 のもう 1 つの利点は、
*エンディアンネス* を考えなくてよいという点です。
UTF-32 には、
バイト列の表現方法によって UTF-32BE と UTF-32LE の 2 種類があり、
また UTF-16 にも同じように、
UTF-16BE と UTF-16LE の 2 種類がありました。
そのため UTF-32 や UTF-16 では、
常に「ビッグエンディアンか?
リトルエンディアンか?」
を意識しなければなりません。
しかし UTF-8 は 8 ビット単位の文字エンコーディングなので、
エンディアンネスを気にする必要がありません。

UTF-8 は数多くの利点を持っていて、
現在、
世界中で広く利用されている文字エンコーディングです。
ただ、
UTF-8 は、
UTF-32 や UTF-16 と比べるといくぶん複雑です。

Unicode のコードポイントは U+0000〜U+10FFFF の値を取ります。
これらの値を 8 ビットのバイト列で表現するために、
UTF-8 の各文字は *1 バイト〜4 バイトの可変長* になっています。

UTF-8 における、
Unicode のコードポイントとバイト列の対応を表 [[tab:char/utf-8-seq]] に示します。

#+caption: UTF-8 のバイト列
#+label: tab:char/utf-8-seq
| コードポイント     | 1 バイト目 | 2 バイト目 | 3 バイト目 | 4 バイト目 |
|--------------------+------------+------------+------------+------------|
| U+0000..U+007F     | 00..7F     |            |            |            |
| U+0080..U+07FF     | C2..DF     | 80..BF     |            |            |
| U+0800..U+0FFF     | E0         | A0..BF     | 80..BF     |            |
| U+1000..U+CFFF     | E1..EC     | 80..BF     | 80..BF     |            |
| U+D000..U+D7FF     | ED         | 80..9F     | 80..BF     |            |
| U+E000..U+FFFF     | EE..EF     | 80..BF     | 80..BF     |            |
| U+10000..U+3FFFF   | F0         | 90..BF     | 80..BF     | 80..BF     |
| U+40000..U+FFFFF   | F1..F3     | 80..BF     | 80..BF     | 80..BF     |
| U+100000..U+10FFFF | F4         | 80..8F     | 80..BF     | 80..BF     |

まず、
コードポイント U+0000〜U+007F の ASCII は、
そのまま 0x00〜0x7f の 1 バイトとして表現されます。

次に、
0 番目のプレーン (BMP) のコードポイント U+0080〜U+07FF に配置されているラテン文字、
ギリシャ文字、
キリル文字、
アルメニア文字、
アラビア文字、
ヘブライ文字などの文字は 2 バイトで表現され、
1 バイト目が 0xc2〜0xdf の値を取ります。
つまり、
コードポイント U+0080〜U+07FF の文字を 2 バイト (16 ビット) で表現します。
1 バイト目が 0xc2〜0xdf のいずれかであれば、
計 2 バイトであることがわかります。

しかし残念ながら、
ひらがな・カタカナ・漢字などはコードポイント U+0080〜U+ 07FF の範囲には配置されていないので、
UTF-8 で、
ひらがな・カタカナ・漢字を表現するためには *最低でも 3 バイト必要* です。

0 番目のプレーン (BMP) に含まれているコードポイント U+0800〜U+FFFF の文字は、
1 バイト目が 0xe0〜0xef の値を取り、
2 バイト目および 3 バイト目が 0x80〜0xbf の値を取ります。
つまり、
コードポイント U+800〜U+FFFF の文字を 3 バイト (24 ビット) で表現します。
1 バイト目が 0xe0〜0xef のいずれかであれば、
計 3 バイトであることがわかるようになっています。

ひらがな・カタカナ・漢字の大部分は 0 番目のプレーン (BMP) に配置されていて、
0 番目のプレーンのコードポイントは 16 ビット (U+0000〜U+FFFF) です。
UTF-8 では、
16 ビットで表現できるひらがな・カタカナ・漢字を、
すべて 24 ビットで表現しています。
このため、
日本語が中心のテキストを表現する場合は、
UTF-8 よりも UTF-16 のほうが記憶領域の大きさという観点では効率的です。

1 番目以降のプレーンのコードポイント U+10000〜U+10FFFF は 4 バイトを使って表現します。

UTF-8 では、
1 バイト目と 2 バイト目以降のビットパターンを区別できるようにうまく選んでいるため、
UTF-8 でエンコードされたバイト列の扱いが比較的簡単です。
例えば、
UTF-8 でエンコードされたバイト列中のある特定のバイトを見たとします。
この値を X (0x00〜0xff のいずれか) とすれば、
以下のように判定できます。

- X の値が 0x00〜0x7f のいずれかであれば、それは U+0000〜U+007F の文字
  であることがわかる
- X の値が 0x80〜0xbf のいずれかであれば、それは 2 バイト目以降の値で
  あることがわかる。前方に 0x80〜0xbf ではないバイトを探し、そのバイト
  を先頭とする (2 バイト以上の) 文字となる
- X の値が 0xc2〜0xdf のいずれかであれば、後続する 1 バイトとあわせて
  U+0080〜U+07FF の文字であることがわかる
- X の値が 0xe0〜0xef のいずれかであれば、後続する 2 バイトとあわせて
  U+0800〜U+FFFF の文字であることがわかる
- X の値が 0xf0〜0xf4 のいずれかであれば、後続する 3 バイトとあわせて
  U+10000〜U+10FFFF の文字であることがわかる
- X の値が上記以外の値であれば、それは UTF-8 として不正な値であること
  がわかる

#+begin_note
本書の原稿は Emacs の org-mode で書いています。
大部分はプレーンテキストですが、
一部 org-mode や LaTeX のコマンドが埋め込まれています。
ある時点での原稿を UTF-8、
UTF-16、
UTF-32 で保存したところ以下のようなファイルサイズになりました。

| 文字エンコーディング | ファイルの大きさ (バイト) | 
|----------------------+---------------------------|
| UTF-8                |                 1 000 955 |
| UTF-16               |                 1 102 998 |
| UTF-32               |                 2 206 052 |

日本語の文書なので、
ひらがな・カタカナ・漢字が中心ですが、
UTF-16 と比較したときの UTF-8 のファイルサイズのオーバーヘッドは 10% 程度でした。

テキストファイルはデータ圧縮が効きやすいデータのため、
ファイルサイズが気になる場合は別途圧縮するという方法もあるでしょう。
参考までに、
~7zr~ コマンドでアーカイブしたときのファイルサイズは以下のようになりました。

| 文字エンコーディング | ファイルの大きさ (バイト) |
|----------------------+---------------------------|
| UTF-8                |                   189 706 |
| UTF-16               |                   181 300 |
| UTF-32               |                   195 410 |
#+end_note

*** BOM (Byte Order Mark)
    
UTF-32、
UTF-16、
UTF-8 はよく考えられた文字エンコーディング体系ですが、
以下のような問題が残っています。

- UTF-32 はエンディアンネスの違いによって、UTF-32BE と UTF-32LE の 2種
  類がある。エンコーディングが UTF-32BE なのか UTF-32LE なのかはバイト
  列だけを見てもわからない
- 同様に、UTF-16 もエンディアンネスの違いによって、UTF-16BE とUTF-16LE
  の 2 種類がある。エンコーディングが UTF-16BE なのかUTF-16LE なのかは
  バイト列だけを見てもわからない
- UTF-32、UTF-16、UTF-8 のいずれかでエンコードされたバイト列を見ても、
  それがどの文字エンコード体系でエンコードされたのかわからない
- したがって、UTF-32、UTF-16、UTF-8 等でエンコードしたバイト列以外に、
  文字エンコーディング体系の種別を表す情報が別途必要になる

Unicode では、
こういった問題を解消するために、
テキストの先頭に *BOM (Byte Order Mark)* と呼ばれる特殊な文字を埋め込むことができます。
Unicode の特殊文字 BOM (コードポイント U+FEFF) を Unicode テキストの先頭に埋め込んでおくと、
UTF-32、
UTF-16、
UTF-8 等でエンコードしたバイト列から、
「このバイト列はどの文字エンコーディング体系でエンコードされたものか?」
がわかるようになります。
このため、
BOM は *Unicode シグネチャ (signature)* とも呼ばれます。

BOM のコードポイントは *U+FEFF* です。
0 番目のプレーン (BMP) の末尾のほうに配置されています。
0xfe と 0xff という、
通常のテキストファイル中にあまり登場しなさそうな値の組み合わせであるというのがポイントです。

BOM を、
UTF-32、
UTF-16、
UTF-8 でエンコードすると以下のようなバイト列になります。

#+caption: BOM (Byte Order Mark) (U+FEFF) のバイト列
#+label: tab:char/bom
| 文字エンコーディング | バイト列            |
|----------------------+---------------------|
| UTF-32BE             | 0x00 0x00 0xfe 0xff |
| UTF-32L              | 0xff 0xfe 0x00 0x00 |
| UTF-16BE             | 0xfe 0xff           |
| UTF-16LE             | 0xff 0xfe           |
| UTF-8                | 0xef 0xb8 0xbf      |

したがって、
「Unicode でエンコードされたバイト列が手元にあるけれど、
どのエンコーディングが使用されたかはわからない」というときには、
*先頭の 2〜4 バイト* をチェックします。
先頭の 2〜4 バイトが上の表の値と一致していれば、
- このバイト列は、先頭に特殊文字 BOM が埋め込まれた Unicode テキストで
  あること
- どの文字エンコーディング体系がエンコーディングに使用されたか
がわかります。

しかも、
BOM はさらに以下のような好ましい性質を持っています。

- BOM のバイト列 (表 [[tab:char/bom]]) は、Unicode 以外の主要な文字エンコー
  ディング (ASCII、ISO-8859、EUC-JP、シフト JIS など) では *めったに
  使用されない*
- そのため、先頭の 2〜4 バイトが BOM のバイト列に一致すれば、「このバ
  イト列は Unicode でエンコードされているだろう」と推測できる

ただし、
*BOM はオプションである* ことには注意が必要です。
Unicode でエンコードされたバイト列であっても、
先頭に BOM がないことも珍しくありません。

また、
歴史的経緯により、
コードポイント U+FEFF は「幅がゼロの改行できない空白 (zero width no-break space)」の意味も持っていることに注意が必要です。
つまり、
Unicode テキストの先頭に U+FEFF があれば、
それは BOM を意味しますが、
Unicode テキストの 2 文字目以降に U+FEFF があれば、
それは「幅がゼロの改行できない空白」を意味するのです。

Unicode でエンコードされたバイト列を処理するプログラムを作成する場合には、
(1) 先頭の 4 バイトが BOM のバイト列に一致すれば、
BOM のバイト列から判定される文字エンコーディング体系を使う、
(2) 先頭の 4 バイトが BOM のバイト列に一致しなければ、
何らかの方法で文字エンコーディング体系を判定する (例: 残りのバイト列から推測する)、
というロジックを組む必要があります。

また、
「Unicode テキストの先頭の U+FEFF はテキストの一部と見なさない」というルールもあります。
このため、
Unicode でエンコードされたバイト列を処理するプログラムを作成する場合は、
バイト列の先頭に U+FEFF があれば、
「幅がゼロの改行できない空白」として扱うのではなく、
テキストから取り除かなければなりません。

BOM はよく考えられた仕組みなのですが、
しばしばトラブルの原因にもなります。
特に、
Unicode における BOM の存在や特徴を十分に認識していない人が、
Unicode でエンコードされたバイト列を処理するプログラムを書くと、
原因不明のトラブルに悩まされることになります (筆者も過去に何度か悩まされました)。

ここで、
BOM によって生じる原因不明のトラブルを実演してみましょう。

dog、
cat、
mouse という英単語が書かれたテキストファイル ~animals.txt~ です。

#+include: "code/char/animals.txt" src raw

~cat~ コマンドでファイルの中身を確認します。
#+begin_src sh
$ cat animals.txt
dog
cat
mouse
#+end_src

~sort~ コマンドで並び換えます。
#+begin_src sh
$ sort animals.txt
cat
dog
mouse
#+end_src
ASCII の文字コード順に、
cat、
dog、
mouse の順番に並び換えられました。

~grep~ コマンドを使って、
アルファベットの a〜f で始まる単語を抽出します。
#+begin_src sh
$ grep '^[a-f]' animals.txt
dog
cat
#+end_src
dog と cat が抽出できました。

dog、
cat、
mouse という英単語が書かれたもう 1 つのテキストファイル ~animals-with-bom.txt~ です。

#+include: "code/char/animals-with-bom.txt" src raw

~cat~ コマンドでファイルの中身を確認します。
#+begin_src sh
$ cat animals-with-bom.txt
dog
cat
mouse
#+end_src
先ほどの ~animals.txt~ とまったく同じであることがわかります。

先ほどと同じように、
~sort~ コマンドで並び換えます。
#+begin_src sh
$ sort animals-with-bom.txt
cat
mouse
dog
#+end_src
ASCII の文字コード順に、
cat、
dog、
mouse の順番に並び換えられて……いません ! なぜか、
cat、
mouse、
dog の順番になっています !?

これも先ほどと同じように、
~grep~ コマンドを使って、
アルファベットの a〜f で始まる単語を抽出します。
#+begin_src sh
$ grep '^[a-f]' animals-with-bom.txt
cat
#+end_src
dog と cat が抽出できて……いません ! なぜか、
cat のみが抽出され、
dog が抽出できていません!?

これが BOM によって生じる (一見すると原因不明の) トラブルの例です。

~sort~ コマンドでソートしても、
正しい順番にソートされません。
~grep~ コマンドで抽出しても、
正しく抽出されません。
~cat~ コマンドを使って、
2 つのファイル (~animals.txt~ と ~animals-with-bom.txt~) と中身が同じであることを確認しました。
上記の実行結果を見ると、
「~sort~ コマンドには、
正しくソートされないことがあるというバグが存在する」としか思えません。

しかし実際には、
上記のトラブルは「目に見えない BOM」によって引き起こされています。

2 つのファイルを ~cat~ コマンドで表示しても、
テキストエディタで開いても、
完全に同一にしか見えません。
しかし、
16 進数でダンプするとその違いがわかります。

#+begin_src sh
$ hd animals.txt 
00000000  64 6f 67 0a 63 61 74 0a  6d 6f 75 73 65 0a        |dog.cat.mouse.|
0000000e
$ hd animals-with-bom.txt 
00000000  ef bb bf 64 6f 67 0a 63  61 74 0a 6d 6f 75 73 65  |...dog.cat.mouse|
00000010  0a                                                |.|
00000011
#+end_src
後者の ~animals-with-bom.txt~ は、
先頭に 0xef、
0xbb、
0xbf という 3 バイトが追加されています。
表 [[tab:char/bom]] から、
これはエンコーディングが UTF-8 の Unicode テキスト (BOM あり) であることがわかります。

このため、
~animals-with-bom.txt~ は、
dog という 3 文字の前に 0xef、
0xbb、
0xbf が存在しています。
~sort~ コマンドや ~grep~ コマンドは BOM を削除しないため、
| ef bb bf 64 6f 67 | 0xef、0xbb、0xbf + 'dog' の 6 文字 |
| 63 61 74          | 'cat' の 3 文字                    |
| 6d 6f 75 73 65    | 'mouse' の 5 文字                  |
という 3 行として取り扱います。

これらの 3 行を ~sort~ コマンドでソートすると
| 63 61 74          | 'cat' の 3 文字                    |
| 6d 6f 75 73 65    | 'mouse' の 5 文字                  |
| ef bb bf 64 6f 67 | 0xef、0xbb、0xbf + 'dog' の 6 文字 |
の順番になります。

同様に、
これらの 3 行から先頭が a (0x61)〜f (0x66) で始まる行を ~grep~ コマンドで抽出すると
| 63 61 74          | 'cat' の 3 文字                    |
だけが抽出されます。

「~animals-with-bom.txt~ は先頭に BOM が付いたテキストファイルかも?」
という疑いを持っていれば、
確認する方法はいろいろあります。

例えば、
2 つのファイルはサイズが異なります。
#+begin_src sh
$ ls -al animals*txt
-rw-r--r-- 1 asm asm 17 Apr 24 09:45 animals-with-bom.txt
-rw-r--r-- 1 asm asm 14 Apr 24 09:44 animals.txt
                     == ファイルサイズ (バイト)
#+end_src

また、
~file~ コマンドでファイルの種別を確認すると、
~animals-with-bom.txt~ が BOM 付きであることがわかります。
#+begin_src sh
$ file animals*txt
animals-with-bom.txt: UTF-8 Unicode (with BOM) text
animals.txt:          ASCII text
#+end_src

ただし、
こういった違いに気付けるのは、
「BOM が原因ではないか」とわかっている場合に限られます。
Unicode における BOM の存在や特徴を十分に認識していない人が、
上記のようなトラブルの原因を特定するのは非常に困難です。

上の例では 3 行のテキストファイルでしたが、
実際にプログラムを書いているときには数千行以上のデータを扱っていることも珍しくありません。
BOM のコードポイントは U+FEFF で、
これは「幅がゼロの改行できない空白」なので、
表示・印刷しても何も見えません。
画面に表示しても、
テキストエディタで読み込んでも、
BOM は「見えない」のです。

さらに、
上の例のように、
一見すると ASCII 文字だけで構成されるテキストファイルに見える場合でも、
「実は BOM が付いた UTF-8 である」可能性があることにも注意してください。
日本語が含まれているテキストファイルであれば、
何かトラブルがあったときには、
「もしかして文字エンコーディングの問題だろうか?」
と疑うことができます。
しかし、
ASCII 文字だけで構成されるような (マルチバイト文字を含まない) テキストにしか見えないときに、
「もしかして文字エンコーディングの問題だろうか?」
と疑うのはかなり困難です。

テキストファイルやテキストのバイト列を扱うときには、
#+begin_quote
先頭に BOM のバイト列があるかもしれない、 \\
先頭に BOM のバイト列があるかもしれない、 \\
先頭に BOM のバイト列があるかもしれない……
#+end_quote
ということを常に念仏のように唱えながらプログラムを書くようにしてください。

#+begin_note
前述のように、
Unicode では、
テキストの先頭の U+FEFF は BOM として扱い、
Unicode のテキストの一部と見なしません。
したがって、
Unicode でエンコードされたバイト列を扱うプログラムを作成する場合には、
テキストの先頭の BOM を削除する必要があります。

先ほどの ~sed~ コマンドや ~grep~ コマンドを使った例では、
入力ファイルが BOM 付きの Unicode テキストであれば、
先頭の BOM を取り除いてやる必要があります。

さまざまな方法がありますが、
例えば ~nkf~ コマンドを使用して
#+begin_src sh
$ nkf animals-with-bom.txt | sort
cat
dog
mouse
#+end_src
とすれば、
ファイル ~animals-with-bom.txt~ の BOM が取り除かれ、
正しくソートされます。
~nkf~ コマンドは、
デフォルトでは、
指定されたファイルを BOM なしの UTF-8 に変換します。

他にも ~sed~ コマンドを使用して、
#+begin_src sh
$ sed $'1s/^\ufeff//' animals-with-bom.txt  | sort
cat
dog
mouse
#+end_src
とする方法もあります。
1 行目の冒頭にあるコードポイント U+FEFF の文字を空文字に置換しています。
ここでは、
~bash~ の ~$'文字列'~ 形式のクォートを利用して U+FEFF を記述しています。

シェルが ~fish~ であれば
#+begin_src sh
> sed "1s/^\ufeff//" animals-with-bom.txt  | sort
cat
dog
mouse
#+end_src
のように記述します。
#+end_note

** 章末問題
<<sec:char/quiz>>

1. 国語辞典 (中規模以上のものが望ましい) で「文字」、「文字列」の項を
   それぞれ調べよ。また、コンピュータにおけるおける「文字」、「文字列」
   に該当する解説を (解説の有無も含めて) 確認せよ。

2. 英英辞典 (中規模以上のものが望ましい) で「character」および「string」
   の項をそれぞれ調べよ。また、コンピュータにおけるおける「文字」、
   「文字列」に該当する解説を (解説の有無も含めて) 確認せよ。

3. 使用しているオペレーティングシステムの言語設定を「英語 (米国)」、
   「フランス」にそれぞれ切り替えてみよ。これによって数値の表記がどの
   ように変わるか (もしくは変わらないか) を調べよ。

4. ASCII で、「ESC (エスケープ)」、「7」、「G」、「g」の文字コードをそ
   れぞれ 16 進数で答えよ。

   #+begin_answer
   0x1b、
   0x37、
   0x47、
   0x67
   #+end_answer

5. ASCII で 0x33、0x55、0x77、0x99、0xbb はそれぞれどのような文字か。

   #+begin_answer
   0x33、
   0x55、
   0x77 はそれぞれ「3」、
   「U」、
   「w」。
   0x99 と 0xbb は ASCII ではない。
   #+end_answer

6. ASCII で 1000010\posn{2} 1111000\posn{2} 1000101\posn{2} はど
   のような文字列か。

   #+begin_answer
   「BYE」
   #+end_answer

7. 自分の名前を ASCII でエンコーディングした時のバイト列を 16 進数で答
   えよ。

8. ASCII における「番号記号」、「ドル記号」、「アンパサンド」、「アス
   テリスク (アスタリスク)」、「コロン」、「セミコロン」、「縦線」はそ
   れぞれどのような形状の文字かを図示せよ。

   #+begin_answer
   「~#~」、
   「~$~」、
   「~&~」、
   「~*~」、
   「~:~」、
   「~;~」、
   「~|~」
   #+end_answer

9. 可変長の文字列を表現する方法として、終端文字を使うのではなく、「文
   字列長 (整数)」と「文字列」によって表現する方法もある。文字列長を使っ
   た表現方法の利点・欠点は何か。

   #+begin_answer
   (利点) 文字列処理が高速にできる。
   
   (欠点) 「文字列長」を固定長のビット列で表現すると、
   短い文字列の表現に無駄が多くなる (例: 文字列長 32 ビット + 文字 8 ビット)。
   さらに、
   最大文字列長がそこで制限される。
   また、
   一方「文字列長」を可変長のビット列で表現すると、
   「文字列長」の解釈や変更の処理が複雑になってしまう。
   #+end_answer

10. 0x00〜0x1f の制御文字列をそれぞれ 1 文字ずつ端末エミュレータで表示
    させよ。使用している端末エミュレータで、それぞれの制御文字の表示に
    よって何が起こったか (例: 0x07 を表示するビープ音が鳴った) を答え
    よ。

11. 画面の 70 カラム目に A と表示する一つの方法は、69 個のスペース
    (0x20) と A (0x41) の合計 70 バイトを表示する方法である。タブ、空
    白、A を組み合わせると、表示に必要なバイト数を何バイトまで削減でき
    るかを答えよ。ただし、タブストップを 8 とする。

    #+begin_answer
    14 バイト (タブ \times 8 + 空白 \times 5 + A)。
    #+end_answer

12. 何らかのテキストファイルを入手し、そのテキストファイルの改行コード
    が何かを、既存のツールを使って調べよ。

    #+begin_answer
    UNIX 系オペレーティングシステムであれば、
    例えば ~file~ コマンドで判定できる。
    #+begin_src sh
    $ file foo
    foo: ASCII text ← 改行コードが LF なら何も表示されない
    $ file bar
    bar: ASCII text, with CRLF line terminators ← 改行コードが CRLF である
    #+end_src
    #+end_answer

13. テキストファイルの改行コードが、「LF のみ」、「CR + LF」のどちらか
    かを判定するプログラム (改行コード判定プログラム) を作成せよ。

    #+begin_answer
    判定アルゴリズムの例: ファイルから 1 行読み込み、
    行末が LF のみか CR + LF かをチェックする。
    
    プログラムの例: ~guess-newline.c~ (図 [[fig:char/ex/guess-newline.c]])
    #+caption: char/ex/guess-newline.c
    #+label: fig:char/ex/guess-newline.c
    #+include: "code/char/ex/guess-newline.c" src C
    
    プログラムの実行例:
#+begin_src sh
$ gcc -o guess-newline guess-newline.c
$ ./guess-newline </tmp/foo
LF
$ ./guess-newline </tmp/bar
CRLF
#+end_src
    #+end_answer

14. 使用しているテキストエディタで、制御文字 (例: フォームフィード
    0x0c) をどうすれば入力できるかを調べよ。

    #+begin_answer
    例えば Emacs であれば C-q で入力できる (例: C-q C-l で ~^L~ が入力できる)。
    #+end_answer

15. 使用しているシェルで、制御文字 (例: 垂直タブ 0x0b) をどうすれば入
    力できるかを調べよ。

    #+begin_answer
    bash であれば、
    Emacs と同じように C-q で入力できる (例: C-q C-l で ~^L~ が入力できる)。
    また、
    ~$'文字列'~ によって C 言語と同じようなエスケープ文字が使用できる (例: フォームフィード ~$'\f'~)。
    #+end_answer

16. ある文字のコードポイントを確認する方法を調査せよ。その方法を用いて、
    「さ」、「サ」、「ざ」、「ザ」のコードポイントを確認せよ。

    #+begin_answer
    U+3055、
    U+30b5、
    U+3056、
    U+30b6
    
    いろいろな方法があるが、
    例えば、
    「さサざザ」とだけ書いたテキストファイルを UTF-32BE に変換し、
    バイナリエディタでダンプするというのが一つの方法である。
#+begin_src sh
$ cat foo 
さサざザ
$ file foo
foo: UTF-8 Unicode text
$ iconv -f UTF-8 -t UTF-32BE foo | hd
00000000  00 00 30 55 00 00 30 b5  00 00 30 56 00 00 30 b6  |..0U..0...0V..0.|
00000010  00 00 00 0a                                       |....|
00000014
#+end_src
    
    他にも、
    Python の関数 ~ord~ で Unicode のコードポイントが取得できる。
#+begin_src python
$ python3
Python 3.9.2 (default, Feb 28 2021, 17:03:44) 
[GCC 10.2.1 20210110] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> hex(ord('さ'))
'0x3055'
>>> 
$ python2
Python 2.7.18 (default, Jul 14 2021, 08:11:37) 
[GCC 10.2.1 20210110] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> hex(ord(u'さ'))
'0x3055'
>>> 
#+end_src
    #+end_answer

17. 日本語が書かれた何らかのテキストファイルを入手し、それを既存のツー
    ルを使って UTF-32BE、UTF-16BE、UTF-8 にそれぞれ変換せよ。変換後の
    ファイルのファイルサイズを比較せよ。

    #+begin_answer
    これもいろいろな方法があるが、
    ~iconv~ コマンドや ~nkf~ コマンドを使えば以下のように変換できる。
    文字エンコーディングがシフト JIS のテキストファイル ~text-in-sjis.txt~ を、
    UTF-8 および UTF-32BE (BOM なし) に変換する例を示す。
    
#+begin_src sh
$ file text-in-sjis.txt 
text-in-sjis.txt: Non-ISO extended-ASCII text
$ iconv -f sjis -t UTF-8 text-in-sjis.txt ← UTF-8 に変換
これは文字エンコーディングがシフト JIS のテキストファイルです。
$ nkf -w text-in-sjis.txt ← UTF-8 に変換 (入力の文字エンコーディングは自動判定)
これは文字エンコーディングがシフト JIS のテキストファイルです。
$ iconv -f sjis -t UTF-32BE text-in-sjis.txt | hd ← UTF-32BE に変換
00000000  00 00 30 53 00 00 30 8c  00 00 30 6f 00 00 65 87  |..0S..0...0o..e.|
00000010  00 00 5b 57 00 00 30 a8  00 00 30 f3 00 00 30 b3  |..[W..0...0...0.|
00000020  00 00 30 fc 00 00 30 c7  00 00 30 a3 00 00 30 f3  |..0...0...0...0.|
00000030  00 00 30 b0 00 00 30 4c  00 00 30 b7 00 00 30 d5  |..0...0L..0...0.|
00000040  00 00 30 c8 00 00 00 20  00 00 00 4a 00 00 00 49  |..0.... ...J...I|
00000050  00 00 00 53 00 00 00 20  00 00 30 6e 00 00 30 c6  |...S... ..0n..0.|
00000060  00 00 30 ad 00 00 30 b9  00 00 30 c8 00 00 30 d5  |..0...0...0...0.|
00000070  00 00 30 a1 00 00 30 a4  00 00 30 eb 00 00 30 67  |..0...0...0...0g|
00000080  00 00 30 59 00 00 30 02  00 00 00 0a              |..0Y..0.....|
0000008c
$ nkf -w320 text-in-sjis.txt | hd ← UTF-32BE に変換
00000000  00 00 30 53 00 00 30 8c  00 00 30 6f 00 00 65 87  |..0S..0...0o..e.|
00000010  00 00 5b 57 00 00 30 a8  00 00 30 f3 00 00 30 b3  |..[W..0...0...0.|
00000020  00 00 30 fc 00 00 30 c7  00 00 30 a3 00 00 30 f3  |..0...0...0...0.|
00000030  00 00 30 b0 00 00 30 4c  00 00 30 b7 00 00 30 d5  |..0...0L..0...0.|
00000040  00 00 30 c8 00 00 00 20  00 00 00 4a 00 00 00 49  |..0.... ...J...I|
00000050  00 00 00 53 00 00 00 20  00 00 30 6e 00 00 30 c6  |...S... ..0n..0.|
00000060  00 00 30 ad 00 00 30 b9  00 00 30 c8 00 00 30 d5  |..0...0...0...0.|
00000070  00 00 30 a1 00 00 30 a4  00 00 30 eb 00 00 30 67  |..0...0...0...0g|
00000080  00 00 30 59 00 00 30 02  00 00 00 0a              |..0Y..0.....|
0000008c
#+end_src
    #+end_answer

18. 自分の名前を UTF-8 (BOM なし) でエンコーディングした時のバイト列を
    16 進数で答えよ。

19. UTF-32BE のバイト列 00 00 6b 63 00 00 89 e3 00 00 30 67 00 00 30
    59 が表す文字列を答えよ。

    #+begin_answer
    「正解です」
    
    いろいろな方法があるが、
    例として、
    Python の関数 ~chr~ や文字列のメソッド ~decode~ を使う方法を示す。
#+begin_src python
$ python3
Python 3.9.2 (default, Feb 28 2021, 17:03:44) 
[GCC 10.2.1 20210110] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> chr(0x6b63)+chr(0x89e3)+chr(0x3067)+chr(0x3059)
'正解です'
>> b'\x00\x00\x6b\x63\x00\x00\x89\xe3\x00\x00\x30\x67\x00\x00\x30\x59'.decode('utf-32be')
'正解です'
#+end_src
    #+end_answer

20. バイト列 fe ff 00 55 00 54 00 46 00 2d 00 31 00 36 30 67 30 59 の
    文字エンコーディングは何か。また、符号化されている文字列が何かを答
    えよ。

    #+begin_answer
    先頭の BOM から UTF-16BE であることわかる。
    「UTF-16 です」。
    
    いろいろな方法があるが、
    例として、
    Python における文字列のメソッド ~decode~ を使う方法を示す。
#+begin_src python
$ python3
Python 3.9.2 (default, Feb 28 2021, 17:03:44) 
[GCC 10.2.1 20210110] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> b'\x00\x55\x00\x54\x00\x46\x00\x2d\x00\x31\x00\x36\x30\x67\x30\x59'.decode('utf-16be')
'UTF-16です'
#+end_src
    #+end_answer
